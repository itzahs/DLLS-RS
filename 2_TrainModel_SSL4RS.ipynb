{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itzahs/SSL-for-RS/blob/main/2_TrainModel_SSL4RS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ **Semi-Supervised Learning for Remote Sensing (SSL4RS) Workshop** üõ∞Ô∏è\n",
        "\n",
        "## üìÇ Section 1 - Get Data & Software: Dataset Download & Augmentation\n",
        "## üõ†Ô∏è Section 2 - Train Model: Implementing FixMatch Algorithm with PyTorch\n",
        "## üìä Section 3 - Model Evaluation: Analyzing Accuracy & Computational Cost from Log Files\n",
        "## üìà Section 4 - Model Inference: Inference with Trained Models & Confusion Matrix\n"
      ],
      "metadata": {
        "id": "6ZT8hosUQMrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìö Setting Up the Working Folder & Importing Required Packages\n"
      ],
      "metadata": {
        "id": "G0El0qJbZ7g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Monitor the GPU usage\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JR-1kDgYMX2",
        "outputId": "18b85496-75cc-4637-8bfd-46bab978bf9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 10 19:15:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount google drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzyWCwb4IIkl",
        "outputId": "afbc41c8-326f-4d50-e70d-39de3d8eeb2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set working folder as default\n",
        "%cd \"/content/drive/MyDrive/SSL4RS\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm9aZ1AwZyEm",
        "outputId": "f3855fc9-f027-4b80-e930-9c41d93faf83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SSL4RS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß **Code Modifications and Library Requirements**\n",
        "\n",
        "This section provides an overview of essential code modifications and library requirements to ensure the software operates smoothly. Key points include:\n",
        "\n",
        "#### **Code Adjustments**\n",
        "\n",
        "To enable seamless operation within a Jupyter notebook environment, specific code adjustments are necessary. These adjustments are detailed below:\n",
        "\n",
        "1. **Modifications to `./Classification-SemiCLS/dataset/builder.py`**\n",
        "\n",
        "   - **Ignore `.ipynb_checkpoints`**: In `dataset/builder.py` (lines 82-92), implement code to ignore `.ipynb_checkpoints` as needed.\n",
        "\n",
        "2. **Modifications to `./Classification-SemiCLS/train_semi.py`**\n",
        "\n",
        "   - **Update 'labeled.next()'**: In `train_semi.py` (lines 444 & 450), replace `labeled.next()` with `next(labeled)`.\n",
        "\n",
        "   - **Update 'unlabeled.next'**: In `train_semi.py` (lines 453 & 459), replace `unlabeled.next` with `next(unlabeled)`.\n",
        "\n",
        "#### **Library Dependencies**\n",
        "\n",
        "To ensure compatibility and proper functionality, it is essential to list the required libraries along with their versions.\n",
        "\n"
      ],
      "metadata": {
        "id": "BlotIX9LfFWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifications to ./Classification-SemiCLS/dataset/builder.py\n",
        "builder_path = \"./Classification-SemiCLS/dataset/builder.py\"\n",
        "\n",
        "# Define the content to be added\n",
        "new_content = \"\"\"\n",
        "        import os\n",
        "        # check if .ipynb_checkpoints is in root, and exclude it\n",
        "        root = os.path.join(cfg.root, '')\n",
        "        if os.path.isdir(os.path.join(root, \".ipynb_checkpoints\")):\n",
        "            exclude_dir = os.path.join(root, \".ipynb_checkpoints\")\n",
        "        else:\n",
        "            exclude_dir = None\n",
        "\"\"\"\n",
        "\n",
        "# Read the content of the file\n",
        "with open(builder_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Check if the new content is already present in the file\n",
        "if new_content.strip() not in content:\n",
        "    # Find the location to insert the new content after \"else:\"\n",
        "    insert_index = content.find(\"else:\")\n",
        "    if insert_index == -1:\n",
        "        raise ValueError(\"Failed to find the insertion point after 'else:'.\")\n",
        "\n",
        "    # Find the end of the line for the \"else:\" statement\n",
        "    end_of_line = content.find(\"\\n\", insert_index)\n",
        "    if end_of_line == -1:\n",
        "        raise ValueError(\"Failed to find the end of the line for 'else:'.\")\n",
        "\n",
        "    # Insert the new content after the \"else:\" block and before the next line\n",
        "    modified_content = content[:end_of_line] + \"\\n\" + new_content + content[end_of_line:]\n",
        "\n",
        "    # Write the modified content back to the file\n",
        "    with open(builder_path, 'w') as file:\n",
        "        file.write(modified_content)\n",
        "\n",
        "    print(\"Modifications applied successfully.\")\n",
        "else:\n",
        "    print(\"Content is already present in the file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHqb-r90mVt9",
        "outputId": "6796c521-c070-4739-a4ae-be3e5b4b9d84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modifications applied successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifications to ./Classification-SemiCLS/train_semi.py\n",
        "train_semi_path = \"./Classification-SemiCLS/train_semi.py\"\n",
        "\n",
        "# Read the content of the file\n",
        "with open(train_semi_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Modify lines as needed\n",
        "modified_lines = []\n",
        "for line in lines:\n",
        "    if \"data_x = labeled_iter.next()\" in line:\n",
        "        modified_lines.append(line.replace(\".next()\", \" = next(labeled_iter)\\n\"))\n",
        "    elif \"data_u = unlabeled_iter.next()\" in line:\n",
        "        modified_lines.append(line.replace(\".next()\", \" = next(unlabeled_iter)\\n\"))\n",
        "    elif \"from mmcv import Config\" in line:\n",
        "        modified_lines.append(line.replace(\"from mmcv import Config\", \"from mmengine.config import Config\"))\n",
        "    else:\n",
        "        modified_lines.append(line)\n",
        "\n",
        "# Write the modified content back to the file\n",
        "with open(train_semi_path, 'w') as file:\n",
        "    file.writelines(modified_lines)\n",
        "\n",
        "print(\"Modifications applied successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WUMIsY0PEPI",
        "outputId": "2d4c127d-8a35-43c7-fe66-65062c571e47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modifications applied successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install the required libraries\n",
        "%%capture\n",
        "!pip install mmcv-lite # the version used to be mmcv-full and now it's mmcv-lite\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install apex\n",
        "!pip install tensorboardX\n",
        "!pip install tensorboard\n",
        "!pip install tensorrt\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "-e-YUNcVLDI0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üõ†Ô∏è **Configuration for Training with FixMatch Algorithm**\n",
        "\n",
        "The configuration code defines parameters for training a deep learning model on the UCM dataset using the FixMatch algorithm. Key details include:\n",
        "\n",
        "- **Model Architecture**: The model used is a wideresnet28x2.\n",
        "- **Hardware**: Training occurs on a single GPU with a batch size of 8.\n",
        "- **Labeled Samples**: Only 4 labeled samples per class are used for training.\n",
        "\n",
        "The training process comprises three main components:\n",
        "\n",
        "1. **Train**: This section specifies the algorithm, the number of training steps, and the loss function.\n",
        "2. **Model**: Details about the architecture of the model to be trained are provided.\n",
        "3. **Data**: This section covers the loading and preprocessing of the UCM dataset, including the number of labeled samples, batch size, and data augmentation pipeline.\n",
        "\n",
        "Other configurations encompass options like the learning rate scheduler, exponential moving average (EMA) of model parameters, automatic mixed precision (AMP) optimization, optimizer and logging/checkpointing settings."
      ],
      "metadata": {
        "id": "HLmCPlATO-l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset builder and training syntax\n",
        "\n",
        "For the code to work in a Jupyter notebook environment we need to make two modifications. First, Jupyter notebook creates .ipynb_checkpoints and we need to ignore them and then, make some changes in the code syntax.\n",
        "1. In dataset/builder.py (lines 15): Add import os\n",
        "2. In dataset/builder.py (lines 82-92): add check to ignore .ipynb_checkpoints.\n",
        "3. In train_semi.py (lines 444 & 450): Modify labeled.next() for next(labeled).\n",
        "4. In train_semi.py (lines 453 & 459): Modify unlabeled.next for next(unlabeled).\n"
      ],
      "metadata": {
        "id": "SkFV70X0q9XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the config file for UCM - 4 labeled examples per class - FixMatch\n",
        "\n",
        "%%writefile ./Classification-SemiCLS/configs/fm_ucm.py\n",
        "\n",
        "\"\"\" The Code is under Tencent Youtu Public Rule\"\"\"\n",
        "\n",
        "train = dict(eval_step=1024,\n",
        "             total_steps=1024*20,\n",
        "             trainer=dict(type=\"FixMatch\",\n",
        "                          threshold=0.95,\n",
        "                          T=1.,\n",
        "                          lambda_u=1.,\n",
        "                          loss_x=dict(\n",
        "                              type=\"cross_entropy\",\n",
        "                              reduction=\"mean\"),\n",
        "                          loss_u=dict(\n",
        "                              type=\"cross_entropy\",\n",
        "                              reduction=\"none\"),\n",
        "                          ))\n",
        "num_classes = 21\n",
        "\n",
        "model = dict(\n",
        "     type=\"wideresnet\",\n",
        "     depth=28,\n",
        "     widen_factor=2,\n",
        "     dropout=0,\n",
        "     num_classes=num_classes,\n",
        ")\n",
        "\n",
        "ucm_mean = (0.485, 0.456, 0.406)\n",
        "ucm_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "data = dict(\n",
        "    type=\"MyDataset\",\n",
        "    num_workers=4,\n",
        "    num_labeled=84,\n",
        "    num_classes=num_classes,\n",
        "    batch_size=4,\n",
        "    expand_labels=False,\n",
        "    mu=7,\n",
        "\n",
        "    root=\"./data/UCM/Images\",\n",
        "    labeled_names_file=\"./data/UCM/Images/UCM_train.txt\",\n",
        "    test_names_file=\"./data/UCM/Images/UCM_test.txt\",\n",
        "    lpipelines=[[\n",
        "        dict(type=\"RandomHorizontalFlip\"),\n",
        "        dict(type=\"RandomResizedCrop\", size=224, scale=(0.2, 1.0)),\n",
        "        dict(type=\"ToTensor\"),\n",
        "        dict(type=\"Normalize\", mean=ucm_mean, std=ucm_std)\n",
        "    ]],\n",
        "    upipelinse=[[\n",
        "        dict(type=\"RandomHorizontalFlip\"),\n",
        "        dict(type=\"Resize\", size=256),\n",
        "        dict(type=\"CenterCrop\", size=224),\n",
        "        dict(type=\"ToTensor\"),\n",
        "        dict(type=\"Normalize\", mean=ucm_mean, std=ucm_std)\n",
        "        ],\n",
        "        [\n",
        "        dict(type=\"RandomHorizontalFlip\"),\n",
        "        dict(type=\"RandomResizedCrop\", size=224, scale=(0.2, 1.0)),\n",
        "        dict(type=\"RandAugmentMC\", n=2, m=10),\n",
        "        dict(type=\"ToTensor\"),\n",
        "        dict(type=\"Normalize\", mean=ucm_mean, std=ucm_std)\n",
        "    ]],\n",
        "    vpipeline=[\n",
        "        dict(type=\"Resize\", size=256),\n",
        "        dict(type=\"ToTensor\"),\n",
        "        dict(type=\"Normalize\", mean=ucm_mean, std=ucm_std)\n",
        "    ])\n",
        "\n",
        "scheduler = dict(\n",
        "    type='cosine_schedule_with_warmup',\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=train['total_steps']\n",
        ")\n",
        "\n",
        "ema = dict(use=True, pseudo_with_ema=False, decay=0.999)\n",
        "#apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "#\"See details at https://nvidia.github.io/apex/amp.html\n",
        "amp = dict(use=False, opt_level=\"O1\")\n",
        "\n",
        "log = dict(interval=1)\n",
        "ckpt = dict(interval=1)\n",
        "\n",
        "# optimizer\n",
        "optimizer = dict(type='SGD', lr=0.03, momentum=0.9, weight_decay=0.0001, nesterov=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLR-cMAYFIPC",
        "outputId": "951806ed-7476-4a60-a8ef-c430a4ef06e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./Classification-SemiCLS/configs/fm_ucm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üöÄ **Initiate the Training** üèãÔ∏è‚Äç‚ôÇÔ∏è\n"
      ],
      "metadata": {
        "id": "pS7Arvmk67lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append the parent directory (Classification-SemiCLS) to sys.path:\n",
        "# The list in Python that specifies the directories for modules and packages to import.\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/SSL4RS/Classification-SemiCLS')\n",
        "\n",
        "# Set working folder as default\n",
        "%cd \"/content/drive/MyDrive/SSL4RS/Classification-SemiCLS\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CkUPwgQmLlG",
        "outputId": "60e3a88f-a0b6-4f91-f9cb-d6391c005c9c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SSL4RS/Classification-SemiCLS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Running UCM with Fixmatch baseline\n",
        "!python3 ./train_semi.py --cfg ./configs/fm_ucm.py --gpu-id 0 --out ./results/fixmatch/fm_ucm --seed 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37SQSW9AFaTb",
        "outputId": "b3033944-e1b2-4038-b399-b19bb04125b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-10 19:20:40.185256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-10 19:20:41.077712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-09-10 19:20:43,132 - WARNING - root -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "2023-09-10 19:20:43,133 - INFO - root -   {'cfg': './configs/fm_ucm.py', 'gpu_id': 0, 'out': './results/fixmatch/fm_ucm', 'pretrained': None, 'resume': '', 'seed': 5, 'use_BN': False, 'fp16': False, 'local_rank': -1, 'no_progress': False, 'other_args': '', 'writer': <torch.utils.tensorboard.writer.SummaryWriter object at 0x788cecb551b0>, 'amp': False, 'total_steps': 20480, 'eval_steps': 1024, 'world_size': 1, 'n_gpu': 1, 'device': device(type='cuda', index=0)}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "2023-09-10 19:20:48,669 - INFO - models.wideresnet -   Model: WideResNet 28x2 proj Falsex64\n",
            "2023-09-10 19:20:48,750 - INFO - root -   wideresnet Total params: 1.47M\n",
            "2023-09-10 19:20:49,369 - INFO - root -   ***** Running training *****\n",
            "2023-09-10 19:20:49,369 - INFO - root -     Task = MyDataset\n",
            "2023-09-10 19:20:49,369 - INFO - root -     Num Label = 84\n",
            "2023-09-10 19:20:49,370 - INFO - root -     Num Epochs = 20\n",
            "2023-09-10 19:20:49,370 - INFO - root -     Batch size per GPU = 4\n",
            "2023-09-10 19:20:49,370 - INFO - root -     Total train batch size = 4\n",
            "2023-09-10 19:20:49,370 - INFO - root -     Total optimization steps = 20480\n",
            "2023-09-10 19:20:49,370 - INFO - root -   Config (path: ./configs/fm_ucm.py): {'train': {'eval_step': 1024, 'total_steps': 20480, 'trainer': {'type': 'FixMatch', 'threshold': 0.95, 'T': 1.0, 'lambda_u': 1.0, 'loss_x': {'type': 'cross_entropy', 'reduction': 'mean'}, 'loss_u': {'type': 'cross_entropy', 'reduction': 'none'}, 'amp': False}}, 'num_classes': 21, 'model': {'type': 'wideresnet', 'depth': 28, 'widen_factor': 2, 'dropout': 0, 'num_classes': 21}, 'ucm_mean': (0.485, 0.456, 0.406), 'ucm_std': (0.229, 0.224, 0.225), 'data': {'type': 'MyDataset', 'num_workers': 4, 'num_labeled': 84, 'num_classes': 21, 'batch_size': 4, 'expand_labels': False, 'mu': 7, 'root': './data/UCM/Images', 'labeled_names_file': './data/UCM/Images/UCM_train.txt', 'test_names_file': './data/UCM/Images/UCM_test.txt', 'lpipelines': [[{'type': 'RandomHorizontalFlip'}, {'type': 'RandomResizedCrop', 'size': 224, 'scale': (0.2, 1.0)}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}]], 'upipelinse': [[{'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': 256}, {'type': 'CenterCrop', 'size': 224}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}], [{'type': 'RandomHorizontalFlip'}, {'type': 'RandomResizedCrop', 'size': 224, 'scale': (0.2, 1.0)}, {'type': 'RandAugmentMC', 'n': 2, 'm': 10}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}]], 'vpipeline': [{'type': 'Resize', 'size': 256}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225)}], 'eval_step': 1024}, 'scheduler': {'type': 'cosine_schedule_with_warmup', 'num_warmup_steps': 0, 'num_training_steps': 20480}, 'ema': {'use': True, 'pseudo_with_ema': False, 'decay': 0.999}, 'amp': {'use': False, 'opt_level': 'O1'}, 'log': {'interval': 1}, 'ckpt': {'interval': 1}, 'optimizer': {'type': 'SGD', 'lr': 0.03, 'momentum': 0.9, 'weight_decay': 0.0001, 'nesterov': True}, 'resume': '', 'seed': 5}\n",
            "2023-09-10 19:21:09,069 - INFO - root -   Train Epoch: 1/  20. Iter:    1/1024. LR: 0.0300 batch_time: 19.555 data_time: 16.504 loss: 2.916 loss_x: 2.916 loss_u: 0.000 mask_prob: 0.000 pseudo_acc: 0.000 \n",
            "2023-09-10 19:21:33,357 - INFO - root -   Train Epoch: 1/  20. Iter:    2/1024. LR: 0.0300 batch_time: 21.922 data_time: 19.635 loss: 3.004 loss_x: 3.004 loss_u: 0.000 mask_prob: 0.000 pseudo_acc: 0.000 \n",
            "2023-09-10 19:21:53,134 - INFO - root -   Train Epoch: 1/  20. Iter:    3/1024. LR: 0.0300 batch_time: 21.207 data_time: 19.181 loss: 2.921 loss_x: 2.921 loss_u: 0.000 mask_prob: 0.000 pseudo_acc: 0.000 \n",
            "2023-09-10 19:22:13,351 - INFO - root -   Train Epoch: 1/  20. Iter:    4/1024. LR: 0.0300 batch_time: 20.959 data_time: 19.066 loss: 3.047 loss_x: 3.047 loss_u: 0.000 mask_prob: 0.000 pseudo_acc: 0.000 \n",
            "2023-09-10 19:22:32,880 - INFO - root -   Train Epoch: 1/  20. Iter:    5/1024. LR: 0.0300 batch_time: 20.673 data_time: 18.857 loss: 3.062 loss_x: 3.062 loss_u: 0.000 mask_prob: 0.000 pseudo_acc: 0.000 \n",
            "2023-09-10 19:22:49,449 - INFO - root -   Train Epoch: 1/  20. Iter:    6/1024. LR: 0.0300 batch_time: 19.989 data_time: 18.217 loss: 3.083 loss_x: 3.083 loss_u: 0.000 mask_prob: 0.000 pseudo_acc: 0.000 \n",
            "2023-09-10 19:23:02,546 - INFO - root -   Train Epoch: 1/  20. Iter:    7/1024. LR: 0.0300 batch_time: 19.005 data_time: 17.256 loss: 3.146 loss_x: 3.146 loss_u: 0.000 mask_prob: 0.000 pseudo_acc: 0.000 \n",
            "2023-09-10 19:23:15,737 - INFO - root -   Train Epoch: 1/  20. Iter:    8/1024. LR: 0.0300 batch_time: 18.278 data_time: 16.555 loss: 3.112 loss_x: 3.111 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.000 \n",
            "2023-09-10 19:23:27,592 - INFO - root -   Train Epoch: 1/  20. Iter:    9/1024. LR: 0.0300 batch_time: 17.564 data_time: 15.862 loss: 3.074 loss_x: 3.073 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.000 \n",
            "2023-09-10 19:23:40,022 - INFO - root -   Train Epoch: 1/  20. Iter:   10/1024. LR: 0.0300 batch_time: 17.051 data_time: 15.366 loss: 3.136 loss_x: 3.135 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.000 \n",
            "2023-09-10 19:23:51,726 - INFO - root -   Train Epoch: 1/  20. Iter:   11/1024. LR: 0.0300 batch_time: 16.565 data_time: 14.895 loss: 3.167 loss_x: 3.166 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.000 \n",
            "2023-09-10 19:24:01,960 - INFO - root -   Train Epoch: 1/  20. Iter:   12/1024. LR: 0.0300 batch_time: 16.037 data_time: 14.378 loss: 3.206 loss_x: 3.205 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.000 \n",
            "2023-09-10 19:24:10,305 - INFO - root -   Train Epoch: 1/  20. Iter:   13/1024. LR: 0.0300 batch_time: 15.445 data_time: 13.797 loss: 3.162 loss_x: 3.162 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.000 \n",
            "2023-09-10 19:24:20,145 - INFO - root -   Train Epoch: 1/  20. Iter:   14/1024. LR: 0.0300 batch_time: 15.045 data_time: 13.407 loss: 3.179 loss_x: 3.178 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.000 \n",
            "2023-09-10 19:24:29,422 - INFO - root -   Train Epoch: 1/  20. Iter:   15/1024. LR: 0.0300 batch_time: 14.661 data_time: 13.031 loss: 3.135 loss_x: 3.135 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:24:37,027 - INFO - root -   Train Epoch: 1/  20. Iter:   16/1024. LR: 0.0300 batch_time: 14.220 data_time: 12.596 loss: 3.141 loss_x: 3.141 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:24:46,407 - INFO - root -   Train Epoch: 1/  20. Iter:   17/1024. LR: 0.0300 batch_time: 13.935 data_time: 12.317 loss: 3.137 loss_x: 3.137 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:24:53,032 - INFO - root -   Train Epoch: 1/  20. Iter:   18/1024. LR: 0.0300 batch_time: 13.529 data_time: 11.915 loss: 3.111 loss_x: 3.111 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:24:59,852 - INFO - root -   Train Epoch: 1/  20. Iter:   19/1024. LR: 0.0300 batch_time: 13.176 data_time: 11.565 loss: 3.106 loss_x: 3.105 loss_u: 0.000 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:05,917 - INFO - root -   Train Epoch: 1/  20. Iter:   20/1024. LR: 0.0300 batch_time: 12.820 data_time: 11.213 loss: 3.105 loss_x: 3.104 loss_u: 0.000 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:11,150 - INFO - root -   Train Epoch: 1/  20. Iter:   21/1024. LR: 0.0300 batch_time: 12.459 data_time: 10.855 loss: 3.123 loss_x: 3.122 loss_u: 0.000 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:15,922 - INFO - root -   Train Epoch: 1/  20. Iter:   22/1024. LR: 0.0300 batch_time: 12.109 data_time: 10.509 loss: 3.161 loss_x: 3.160 loss_u: 0.000 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:21,772 - INFO - root -   Train Epoch: 1/  20. Iter:   23/1024. LR: 0.0300 batch_time: 11.837 data_time: 10.239 loss: 3.146 loss_x: 3.146 loss_u: 0.000 mask_prob: 0.002 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:25,806 - INFO - root -   Train Epoch: 1/  20. Iter:   24/1024. LR: 0.0300 batch_time: 11.512 data_time: 9.917 loss: 3.147 loss_x: 3.147 loss_u: 0.000 mask_prob: 0.001 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:30,746 - INFO - root -   Train Epoch: 1/  20. Iter:   25/1024. LR: 0.0300 batch_time: 11.249 data_time: 9.656 loss: 3.146 loss_x: 3.146 loss_u: 0.000 mask_prob: 0.001 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:36,449 - INFO - root -   Train Epoch: 1/  20. Iter:   26/1024. LR: 0.0300 batch_time: 11.036 data_time: 9.445 loss: 3.187 loss_x: 3.186 loss_u: 0.000 mask_prob: 0.001 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:40,741 - INFO - root -   Train Epoch: 1/  20. Iter:   27/1024. LR: 0.0300 batch_time: 10.786 data_time: 9.197 loss: 3.188 loss_x: 3.188 loss_u: 0.000 mask_prob: 0.001 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:45,172 - INFO - root -   Train Epoch: 1/  20. Iter:   28/1024. LR: 0.0300 batch_time: 10.559 data_time: 8.972 loss: 3.220 loss_x: 3.220 loss_u: 0.000 mask_prob: 0.001 pseudo_acc: 0.000 \n",
            "2023-09-10 19:25:49,297 - INFO - root -   Train Epoch: 1/  20. Iter:   29/1024. LR: 0.0300 batch_time: 10.337 data_time: 8.752 loss: 3.253 loss_x: 3.250 loss_u: 0.003 mask_prob: 0.002 pseudo_acc: 0.034 \n",
            "2023-09-10 19:25:53,159 - INFO - root -   Train Epoch: 1/  20. Iter:   30/1024. LR: 0.0300 batch_time: 10.122 data_time: 8.537 loss: 3.248 loss_x: 3.245 loss_u: 0.003 mask_prob: 0.002 pseudo_acc: 0.033 \n",
            "2023-09-10 19:25:56,884 - INFO - root -   Train Epoch: 1/  20. Iter:   31/1024. LR: 0.0300 batch_time: 9.915 data_time: 8.333 loss: 3.214 loss_x: 3.211 loss_u: 0.003 mask_prob: 0.002 pseudo_acc: 0.032 \n",
            "2023-09-10 19:26:01,247 - INFO - root -   Train Epoch: 1/  20. Iter:   32/1024. LR: 0.0300 batch_time: 9.742 data_time: 8.160 loss: 3.190 loss_x: 3.187 loss_u: 0.003 mask_prob: 0.003 pseudo_acc: 0.062 \n",
            "2023-09-10 19:26:05,165 - INFO - root -   Train Epoch: 1/  20. Iter:   33/1024. LR: 0.0300 batch_time: 9.565 data_time: 7.984 loss: 3.191 loss_x: 3.188 loss_u: 0.003 mask_prob: 0.003 pseudo_acc: 0.061 \n",
            "2023-09-10 19:26:08,740 - INFO - root -   Train Epoch: 1/  20. Iter:   34/1024. LR: 0.0300 batch_time: 9.389 data_time: 7.809 loss: 3.189 loss_x: 3.187 loss_u: 0.003 mask_prob: 0.004 pseudo_acc: 0.059 \n",
            "2023-09-10 19:26:12,626 - INFO - root -   Train Epoch: 1/  20. Iter:   35/1024. LR: 0.0300 batch_time: 9.232 data_time: 7.653 loss: 3.199 loss_x: 3.196 loss_u: 0.003 mask_prob: 0.004 pseudo_acc: 0.057 \n",
            "2023-09-10 19:26:17,177 - INFO - root -   Train Epoch: 1/  20. Iter:   36/1024. LR: 0.0300 batch_time: 9.102 data_time: 7.524 loss: 3.170 loss_x: 3.167 loss_u: 0.003 mask_prob: 0.005 pseudo_acc: 0.056 \n",
            "2023-09-10 19:26:20,634 - INFO - root -   Train Epoch: 1/  20. Iter:   37/1024. LR: 0.0300 batch_time: 8.949 data_time: 7.372 loss: 3.194 loss_x: 3.192 loss_u: 0.003 mask_prob: 0.006 pseudo_acc: 0.054 \n",
            "2023-09-10 19:26:23,613 - INFO - root -   Train Epoch: 1/  20. Iter:   38/1024. LR: 0.0300 batch_time: 8.792 data_time: 7.217 loss: 3.192 loss_x: 3.190 loss_u: 0.002 mask_prob: 0.007 pseudo_acc: 0.053 \n",
            "2023-09-10 19:26:27,133 - INFO - root -   Train Epoch: 1/  20. Iter:   39/1024. LR: 0.0300 batch_time: 8.657 data_time: 7.082 loss: 3.195 loss_x: 3.192 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.051 \n",
            "2023-09-10 19:26:31,636 - INFO - root -   Train Epoch: 1/  20. Iter:   40/1024. LR: 0.0300 batch_time: 8.553 data_time: 6.979 loss: 3.268 loss_x: 3.266 loss_u: 0.002 mask_prob: 0.008 pseudo_acc: 0.050 \n",
            "2023-09-10 19:26:34,686 - INFO - root -   Train Epoch: 1/  20. Iter:   41/1024. LR: 0.0300 batch_time: 8.419 data_time: 6.846 loss: 3.286 loss_x: 3.284 loss_u: 0.002 mask_prob: 0.008 pseudo_acc: 0.049 \n",
            "2023-09-10 19:26:37,673 - INFO - root -   Train Epoch: 1/  20. Iter:   42/1024. LR: 0.0300 batch_time: 8.290 data_time: 6.718 loss: 3.303 loss_x: 3.301 loss_u: 0.002 mask_prob: 0.008 pseudo_acc: 0.048 \n",
            "2023-09-10 19:26:40,804 - INFO - root -   Train Epoch: 1/  20. Iter:   43/1024. LR: 0.0300 batch_time: 8.170 data_time: 6.600 loss: 3.292 loss_x: 3.290 loss_u: 0.002 mask_prob: 0.007 pseudo_acc: 0.047 \n",
            "2023-09-10 19:26:44,131 - INFO - root -   Train Epoch: 1/  20. Iter:   44/1024. LR: 0.0300 batch_time: 8.059 data_time: 6.491 loss: 3.311 loss_x: 3.309 loss_u: 0.002 mask_prob: 0.007 pseudo_acc: 0.045 \n",
            "2023-09-10 19:26:48,293 - INFO - root -   Train Epoch: 1/  20. Iter:   45/1024. LR: 0.0300 batch_time: 7.973 data_time: 6.405 loss: 3.324 loss_x: 3.321 loss_u: 0.002 mask_prob: 0.007 pseudo_acc: 0.044 \n",
            "2023-09-10 19:26:51,122 - INFO - root -   Train Epoch: 1/  20. Iter:   46/1024. LR: 0.0300 batch_time: 7.861 data_time: 6.294 loss: 3.348 loss_x: 3.346 loss_u: 0.002 mask_prob: 0.007 pseudo_acc: 0.043 \n",
            "2023-09-10 19:26:54,563 - INFO - root -   Train Epoch: 1/  20. Iter:   47/1024. LR: 0.0300 batch_time: 7.767 data_time: 6.201 loss: 3.352 loss_x: 3.350 loss_u: 0.002 mask_prob: 0.007 pseudo_acc: 0.043 \n",
            "2023-09-10 19:26:58,348 - INFO - root -   Train Epoch: 1/  20. Iter:   48/1024. LR: 0.0300 batch_time: 7.684 data_time: 6.118 loss: 3.352 loss_x: 3.350 loss_u: 0.002 mask_prob: 0.007 pseudo_acc: 0.042 \n",
            "2023-09-10 19:27:01,873 - INFO - root -   Train Epoch: 1/  20. Iter:   49/1024. LR: 0.0300 batch_time: 7.599 data_time: 6.033 loss: 3.344 loss_x: 3.342 loss_u: 0.002 mask_prob: 0.007 pseudo_acc: 0.041 \n",
            "2023-09-10 19:27:04,896 - INFO - root -   Train Epoch: 1/  20. Iter:   50/1024. LR: 0.0300 batch_time: 7.508 data_time: 5.943 loss: 3.335 loss_x: 3.333 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.040 \n",
            "2023-09-10 19:27:07,764 - INFO - root -   Train Epoch: 1/  20. Iter:   51/1024. LR: 0.0300 batch_time: 7.417 data_time: 5.853 loss: 3.319 loss_x: 3.317 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.039 \n",
            "2023-09-10 19:27:10,728 - INFO - root -   Train Epoch: 1/  20. Iter:   52/1024. LR: 0.0300 batch_time: 7.331 data_time: 5.768 loss: 3.307 loss_x: 3.305 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.038 \n",
            "2023-09-10 19:27:14,876 - INFO - root -   Train Epoch: 1/  20. Iter:   53/1024. LR: 0.0300 batch_time: 7.271 data_time: 5.708 loss: 3.299 loss_x: 3.297 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.038 \n",
            "2023-09-10 19:27:17,842 - INFO - root -   Train Epoch: 1/  20. Iter:   54/1024. LR: 0.0300 batch_time: 7.191 data_time: 5.630 loss: 3.303 loss_x: 3.302 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.037 \n",
            "2023-09-10 19:27:20,848 - INFO - root -   Train Epoch: 1/  20. Iter:   55/1024. LR: 0.0300 batch_time: 7.115 data_time: 5.555 loss: 3.311 loss_x: 3.309 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.036 \n",
            "2023-09-10 19:27:23,899 - INFO - root -   Train Epoch: 1/  20. Iter:   56/1024. LR: 0.0300 batch_time: 7.043 data_time: 5.483 loss: 3.297 loss_x: 3.295 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.036 \n",
            "2023-09-10 19:27:26,924 - INFO - root -   Train Epoch: 1/  20. Iter:   57/1024. LR: 0.0300 batch_time: 6.972 data_time: 5.414 loss: 3.287 loss_x: 3.285 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.035 \n",
            "2023-09-10 19:27:30,879 - INFO - root -   Train Epoch: 1/  20. Iter:   58/1024. LR: 0.0300 batch_time: 6.920 data_time: 5.362 loss: 3.279 loss_x: 3.277 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.034 \n",
            "2023-09-10 19:27:34,069 - INFO - root -   Train Epoch: 1/  20. Iter:   59/1024. LR: 0.0300 batch_time: 6.857 data_time: 5.299 loss: 3.284 loss_x: 3.282 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.034 \n",
            "2023-09-10 19:27:37,074 - INFO - root -   Train Epoch: 1/  20. Iter:   60/1024. LR: 0.0300 batch_time: 6.793 data_time: 5.236 loss: 3.301 loss_x: 3.300 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.033 \n",
            "2023-09-10 19:27:40,187 - INFO - root -   Train Epoch: 1/  20. Iter:   61/1024. LR: 0.0300 batch_time: 6.732 data_time: 5.176 loss: 3.288 loss_x: 3.286 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.033 \n",
            "2023-09-10 19:27:44,076 - INFO - root -   Train Epoch: 1/  20. Iter:   62/1024. LR: 0.0300 batch_time: 6.686 data_time: 5.130 loss: 3.277 loss_x: 3.276 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.032 \n",
            "2023-09-10 19:27:46,903 - INFO - root -   Train Epoch: 1/  20. Iter:   63/1024. LR: 0.0300 batch_time: 6.625 data_time: 5.070 loss: 3.273 loss_x: 3.271 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.032 \n",
            "2023-09-10 19:27:49,856 - INFO - root -   Train Epoch: 1/  20. Iter:   64/1024. LR: 0.0300 batch_time: 6.568 data_time: 5.013 loss: 3.290 loss_x: 3.288 loss_u: 0.002 mask_prob: 0.006 pseudo_acc: 0.031 \n",
            "2023-09-10 19:27:52,942 - INFO - root -   Train Epoch: 1/  20. Iter:   65/1024. LR: 0.0300 batch_time: 6.514 data_time: 4.961 loss: 3.300 loss_x: 3.299 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.031 \n",
            "2023-09-10 19:27:56,090 - INFO - root -   Train Epoch: 1/  20. Iter:   66/1024. LR: 0.0300 batch_time: 6.463 data_time: 4.910 loss: 3.285 loss_x: 3.283 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.030 \n",
            "2023-09-10 19:28:00,370 - INFO - root -   Train Epoch: 1/  20. Iter:   67/1024. LR: 0.0300 batch_time: 6.431 data_time: 4.878 loss: 3.283 loss_x: 3.282 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.030 \n",
            "2023-09-10 19:28:03,557 - INFO - root -   Train Epoch: 1/  20. Iter:   68/1024. LR: 0.0300 batch_time: 6.383 data_time: 4.831 loss: 3.274 loss_x: 3.273 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.029 \n",
            "2023-09-10 19:28:06,288 - INFO - root -   Train Epoch: 1/  20. Iter:   69/1024. LR: 0.0300 batch_time: 6.330 data_time: 4.778 loss: 3.268 loss_x: 3.267 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.029 \n",
            "2023-09-10 19:28:09,254 - INFO - root -   Train Epoch: 1/  20. Iter:   70/1024. LR: 0.0300 batch_time: 6.282 data_time: 4.731 loss: 3.268 loss_x: 3.267 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.029 \n",
            "2023-09-10 19:28:13,496 - INFO - root -   Train Epoch: 1/  20. Iter:   71/1024. LR: 0.0300 batch_time: 6.253 data_time: 4.702 loss: 3.284 loss_x: 3.283 loss_u: 0.001 mask_prob: 0.007 pseudo_acc: 0.028 \n",
            "2023-09-10 19:28:16,483 - INFO - root -   Train Epoch: 1/  20. Iter:   72/1024. LR: 0.0300 batch_time: 6.208 data_time: 4.657 loss: 3.282 loss_x: 3.281 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.028 \n",
            "2023-09-10 19:28:19,592 - INFO - root -   Train Epoch: 1/  20. Iter:   73/1024. LR: 0.0300 batch_time: 6.165 data_time: 4.616 loss: 3.289 loss_x: 3.288 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.027 \n",
            "2023-09-10 19:28:22,716 - INFO - root -   Train Epoch: 1/  20. Iter:   74/1024. LR: 0.0300 batch_time: 6.124 data_time: 4.575 loss: 3.285 loss_x: 3.283 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.027 \n",
            "2023-09-10 19:28:26,020 - INFO - root -   Train Epoch: 1/  20. Iter:   75/1024. LR: 0.0300 batch_time: 6.087 data_time: 4.537 loss: 3.300 loss_x: 3.298 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.027 \n",
            "2023-09-10 19:28:29,784 - INFO - root -   Train Epoch: 1/  20. Iter:   76/1024. LR: 0.0300 batch_time: 6.056 data_time: 4.507 loss: 3.294 loss_x: 3.293 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.026 \n",
            "2023-09-10 19:28:32,631 - INFO - root -   Train Epoch: 1/  20. Iter:   77/1024. LR: 0.0300 batch_time: 6.015 data_time: 4.465 loss: 3.289 loss_x: 3.288 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.026 \n",
            "2023-09-10 19:28:35,710 - INFO - root -   Train Epoch: 1/  20. Iter:   78/1024. LR: 0.0300 batch_time: 5.977 data_time: 4.428 loss: 3.290 loss_x: 3.289 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.026 \n",
            "2023-09-10 19:28:38,793 - INFO - root -   Train Epoch: 1/  20. Iter:   79/1024. LR: 0.0300 batch_time: 5.940 data_time: 4.392 loss: 3.285 loss_x: 3.284 loss_u: 0.001 mask_prob: 0.007 pseudo_acc: 0.038 \n",
            "2023-09-10 19:28:43,075 - INFO - root -   Train Epoch: 1/  20. Iter:   80/1024. LR: 0.0300 batch_time: 5.920 data_time: 4.371 loss: 3.277 loss_x: 3.276 loss_u: 0.001 mask_prob: 0.007 pseudo_acc: 0.037 \n",
            "2023-09-10 19:28:45,762 - INFO - root -   Train Epoch: 1/  20. Iter:   81/1024. LR: 0.0300 batch_time: 5.880 data_time: 4.332 loss: 3.277 loss_x: 3.275 loss_u: 0.001 mask_prob: 0.007 pseudo_acc: 0.037 \n",
            "2023-09-10 19:28:48,526 - INFO - root -   Train Epoch: 1/  20. Iter:   82/1024. LR: 0.0300 batch_time: 5.842 data_time: 4.294 loss: 3.276 loss_x: 3.275 loss_u: 0.001 mask_prob: 0.007 pseudo_acc: 0.037 \n",
            "2023-09-10 19:28:51,520 - INFO - root -   Train Epoch: 1/  20. Iter:   83/1024. LR: 0.0300 batch_time: 5.807 data_time: 4.261 loss: 3.270 loss_x: 3.269 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.036 \n",
            "2023-09-10 19:28:54,670 - INFO - root -   Train Epoch: 1/  20. Iter:   84/1024. LR: 0.0300 batch_time: 5.776 data_time: 4.229 loss: 3.260 loss_x: 3.259 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.036 \n",
            "2023-09-10 19:28:58,984 - INFO - root -   Train Epoch: 1/  20. Iter:   85/1024. LR: 0.0300 batch_time: 5.758 data_time: 4.212 loss: 3.258 loss_x: 3.257 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.035 \n",
            "2023-09-10 19:29:01,682 - INFO - root -   Train Epoch: 1/  20. Iter:   86/1024. LR: 0.0300 batch_time: 5.723 data_time: 4.177 loss: 3.256 loss_x: 3.255 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.035 \n",
            "2023-09-10 19:29:04,785 - INFO - root -   Train Epoch: 1/  20. Iter:   87/1024. LR: 0.0300 batch_time: 5.693 data_time: 4.147 loss: 3.249 loss_x: 3.248 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.034 \n",
            "2023-09-10 19:29:07,628 - INFO - root -   Train Epoch: 1/  20. Iter:   88/1024. LR: 0.0300 batch_time: 5.660 data_time: 4.115 loss: 3.247 loss_x: 3.246 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.034 \n",
            "2023-09-10 19:29:11,429 - INFO - root -   Train Epoch: 1/  20. Iter:   89/1024. LR: 0.0300 batch_time: 5.639 data_time: 4.094 loss: 3.244 loss_x: 3.243 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.034 \n",
            "2023-09-10 19:29:14,807 - INFO - root -   Train Epoch: 1/  20. Iter:   90/1024. LR: 0.0300 batch_time: 5.614 data_time: 4.069 loss: 3.245 loss_x: 3.244 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.033 \n",
            "2023-09-10 19:29:17,766 - INFO - root -   Train Epoch: 1/  20. Iter:   91/1024. LR: 0.0300 batch_time: 5.585 data_time: 4.040 loss: 3.240 loss_x: 3.238 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.033 \n",
            "2023-09-10 19:29:20,909 - INFO - root -   Train Epoch: 1/  20. Iter:   92/1024. LR: 0.0300 batch_time: 5.559 data_time: 4.014 loss: 3.235 loss_x: 3.234 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.033 \n",
            "2023-09-10 19:29:23,584 - INFO - root -   Train Epoch: 1/  20. Iter:   93/1024. LR: 0.0300 batch_time: 5.528 data_time: 3.984 loss: 3.236 loss_x: 3.235 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.032 \n",
            "2023-09-10 19:29:27,884 - INFO - root -   Train Epoch: 1/  20. Iter:   94/1024. LR: 0.0300 batch_time: 5.515 data_time: 3.971 loss: 3.231 loss_x: 3.230 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.032 \n",
            "2023-09-10 19:29:30,866 - INFO - root -   Train Epoch: 1/  20. Iter:   95/1024. LR: 0.0300 batch_time: 5.488 data_time: 3.944 loss: 3.226 loss_x: 3.225 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.032 \n",
            "2023-09-10 19:29:34,017 - INFO - root -   Train Epoch: 1/  20. Iter:   96/1024. LR: 0.0300 batch_time: 5.464 data_time: 3.920 loss: 3.233 loss_x: 3.232 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.031 \n",
            "2023-09-10 19:29:36,724 - INFO - root -   Train Epoch: 1/  20. Iter:   97/1024. LR: 0.0300 batch_time: 5.435 data_time: 3.892 loss: 3.229 loss_x: 3.228 loss_u: 0.001 mask_prob: 0.006 pseudo_acc: 0.031 \n",
            "2023-09-10 19:29:40,844 - INFO - root -   Train Epoch: 1/  20. Iter:   98/1024. LR: 0.0300 batch_time: 5.422 data_time: 3.879 loss: 3.261 loss_x: 3.260 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.031 \n",
            "2023-09-10 19:29:44,365 - INFO - root -   Train Epoch: 1/  20. Iter:   99/1024. LR: 0.0300 batch_time: 5.403 data_time: 3.859 loss: 3.258 loss_x: 3.257 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.030 \n",
            "2023-09-10 19:29:47,036 - INFO - root -   Train Epoch: 1/  20. Iter:  100/1024. LR: 0.0300 batch_time: 5.375 data_time: 3.832 loss: 3.256 loss_x: 3.255 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.030 \n",
            "2023-09-10 19:29:50,046 - INFO - root -   Train Epoch: 1/  20. Iter:  101/1024. LR: 0.0300 batch_time: 5.352 data_time: 3.809 loss: 3.253 loss_x: 3.252 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.030 \n",
            "2023-09-10 19:29:52,938 - INFO - root -   Train Epoch: 1/  20. Iter:  102/1024. LR: 0.0300 batch_time: 5.328 data_time: 3.786 loss: 3.249 loss_x: 3.248 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.029 \n",
            "2023-09-10 19:29:56,892 - INFO - root -   Train Epoch: 1/  20. Iter:  103/1024. LR: 0.0300 batch_time: 5.314 data_time: 3.772 loss: 3.246 loss_x: 3.245 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.029 \n",
            "2023-09-10 19:29:59,861 - INFO - root -   Train Epoch: 1/  20. Iter:  104/1024. LR: 0.0300 batch_time: 5.292 data_time: 3.750 loss: 3.265 loss_x: 3.264 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.029 \n",
            "2023-09-10 19:30:03,017 - INFO - root -   Train Epoch: 1/  20. Iter:  105/1024. LR: 0.0300 batch_time: 5.271 data_time: 3.730 loss: 3.262 loss_x: 3.261 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.029 \n",
            "2023-09-10 19:30:06,117 - INFO - root -   Train Epoch: 1/  20. Iter:  106/1024. LR: 0.0300 batch_time: 5.251 data_time: 3.710 loss: 3.285 loss_x: 3.284 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.028 \n",
            "2023-09-10 19:30:10,437 - INFO - root -   Train Epoch: 1/  20. Iter:  107/1024. LR: 0.0300 batch_time: 5.242 data_time: 3.701 loss: 3.280 loss_x: 3.279 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.028 \n",
            "2023-09-10 19:30:13,461 - INFO - root -   Train Epoch: 1/  20. Iter:  108/1024. LR: 0.0300 batch_time: 5.222 data_time: 3.681 loss: 3.275 loss_x: 3.274 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.028 \n",
            "2023-09-10 19:30:16,483 - INFO - root -   Train Epoch: 1/  20. Iter:  109/1024. LR: 0.0300 batch_time: 5.202 data_time: 3.661 loss: 3.274 loss_x: 3.273 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.028 \n",
            "2023-09-10 19:30:19,190 - INFO - root -   Train Epoch: 1/  20. Iter:  110/1024. LR: 0.0300 batch_time: 5.179 data_time: 3.639 loss: 3.275 loss_x: 3.274 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.027 \n",
            "2023-09-10 19:30:22,186 - INFO - root -   Train Epoch: 1/  20. Iter:  111/1024. LR: 0.0300 batch_time: 5.159 data_time: 3.619 loss: 3.271 loss_x: 3.270 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.027 \n",
            "2023-09-10 19:30:26,477 - INFO - root -   Train Epoch: 1/  20. Iter:  112/1024. LR: 0.0300 batch_time: 5.151 data_time: 3.611 loss: 3.268 loss_x: 3.268 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.027 \n",
            "2023-09-10 19:30:29,418 - INFO - root -   Train Epoch: 1/  20. Iter:  113/1024. LR: 0.0300 batch_time: 5.132 data_time: 3.592 loss: 3.264 loss_x: 3.264 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.027 \n",
            "2023-09-10 19:30:32,470 - INFO - root -   Train Epoch: 1/  20. Iter:  114/1024. LR: 0.0300 batch_time: 5.114 data_time: 3.574 loss: 3.260 loss_x: 3.259 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.026 \n",
            "2023-09-10 19:30:35,192 - INFO - root -   Train Epoch: 1/  20. Iter:  115/1024. LR: 0.0300 batch_time: 5.093 data_time: 3.554 loss: 3.257 loss_x: 3.256 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.026 \n",
            "2023-09-10 19:30:39,164 - INFO - root -   Train Epoch: 1/  20. Iter:  116/1024. LR: 0.0300 batch_time: 5.083 data_time: 3.544 loss: 3.249 loss_x: 3.248 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.026 \n",
            "2023-09-10 19:30:42,629 - INFO - root -   Train Epoch: 1/  20. Iter:  117/1024. LR: 0.0300 batch_time: 5.069 data_time: 3.530 loss: 3.245 loss_x: 3.244 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.026 \n",
            "2023-09-10 19:30:45,592 - INFO - root -   Train Epoch: 1/  20. Iter:  118/1024. LR: 0.0300 batch_time: 5.052 data_time: 3.512 loss: 3.241 loss_x: 3.240 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.025 \n",
            "2023-09-10 19:30:48,256 - INFO - root -   Train Epoch: 1/  20. Iter:  119/1024. LR: 0.0300 batch_time: 5.031 data_time: 3.493 loss: 3.239 loss_x: 3.238 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.025 \n",
            "2023-09-10 19:30:51,208 - INFO - root -   Train Epoch: 1/  20. Iter:  120/1024. LR: 0.0300 batch_time: 5.014 data_time: 3.475 loss: 3.241 loss_x: 3.240 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:30:55,530 - INFO - root -   Train Epoch: 1/  20. Iter:  121/1024. LR: 0.0300 batch_time: 5.008 data_time: 3.470 loss: 3.235 loss_x: 3.234 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:30:58,495 - INFO - root -   Train Epoch: 1/  20. Iter:  122/1024. LR: 0.0300 batch_time: 4.992 data_time: 3.453 loss: 3.230 loss_x: 3.230 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:31:01,244 - INFO - root -   Train Epoch: 1/  20. Iter:  123/1024. LR: 0.0300 batch_time: 4.973 data_time: 3.435 loss: 3.221 loss_x: 3.221 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:31:04,093 - INFO - root -   Train Epoch: 1/  20. Iter:  124/1024. LR: 0.0300 batch_time: 4.956 data_time: 3.418 loss: 3.215 loss_x: 3.214 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:31:07,760 - INFO - root -   Train Epoch: 1/  20. Iter:  125/1024. LR: 0.0300 batch_time: 4.946 data_time: 3.408 loss: 3.204 loss_x: 3.203 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:31:11,520 - INFO - root -   Train Epoch: 1/  20. Iter:  126/1024. LR: 0.0300 batch_time: 4.937 data_time: 3.398 loss: 3.206 loss_x: 3.206 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:31:14,313 - INFO - root -   Train Epoch: 1/  20. Iter:  127/1024. LR: 0.0300 batch_time: 4.920 data_time: 3.382 loss: 3.202 loss_x: 3.201 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:31:17,241 - INFO - root -   Train Epoch: 1/  20. Iter:  128/1024. LR: 0.0300 batch_time: 4.904 data_time: 3.367 loss: 3.196 loss_x: 3.196 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:31:19,986 - INFO - root -   Train Epoch: 1/  20. Iter:  129/1024. LR: 0.0300 batch_time: 4.887 data_time: 3.350 loss: 3.202 loss_x: 3.201 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:31:24,319 - INFO - root -   Train Epoch: 1/  20. Iter:  130/1024. LR: 0.0300 batch_time: 4.883 data_time: 3.346 loss: 3.196 loss_x: 3.195 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:31:27,425 - INFO - root -   Train Epoch: 1/  20. Iter:  131/1024. LR: 0.0300 batch_time: 4.870 data_time: 3.332 loss: 3.190 loss_x: 3.189 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:31:30,507 - INFO - root -   Train Epoch: 1/  20. Iter:  132/1024. LR: 0.0300 batch_time: 4.856 data_time: 3.319 loss: 3.188 loss_x: 3.187 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:31:33,311 - INFO - root -   Train Epoch: 1/  20. Iter:  133/1024. LR: 0.0300 batch_time: 4.841 data_time: 3.304 loss: 3.188 loss_x: 3.187 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:31:37,017 - INFO - root -   Train Epoch: 1/  20. Iter:  134/1024. LR: 0.0300 batch_time: 4.832 data_time: 3.295 loss: 3.182 loss_x: 3.182 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.022 \n",
            "2023-09-10 19:31:40,530 - INFO - root -   Train Epoch: 1/  20. Iter:  135/1024. LR: 0.0300 batch_time: 4.822 data_time: 3.285 loss: 3.183 loss_x: 3.182 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.022 \n",
            "2023-09-10 19:31:43,565 - INFO - root -   Train Epoch: 1/  20. Iter:  136/1024. LR: 0.0300 batch_time: 4.809 data_time: 3.272 loss: 3.181 loss_x: 3.180 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.022 \n",
            "2023-09-10 19:31:46,534 - INFO - root -   Train Epoch: 1/  20. Iter:  137/1024. LR: 0.0300 batch_time: 4.796 data_time: 3.259 loss: 3.171 loss_x: 3.170 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.022 \n",
            "2023-09-10 19:31:49,383 - INFO - root -   Train Epoch: 1/  20. Iter:  138/1024. LR: 0.0300 batch_time: 4.782 data_time: 3.245 loss: 3.170 loss_x: 3.169 loss_u: 0.001 mask_prob: 0.004 pseudo_acc: 0.022 \n",
            "2023-09-10 19:31:53,726 - INFO - root -   Train Epoch: 1/  20. Iter:  139/1024. LR: 0.0300 batch_time: 4.779 data_time: 3.242 loss: 3.183 loss_x: 3.183 loss_u: 0.001 mask_prob: 0.005 pseudo_acc: 0.022 \n",
            "2023-09-10 19:31:56,895 - INFO - root -   Train Epoch: 1/  20. Iter:  140/1024. LR: 0.0300 batch_time: 4.767 data_time: 3.231 loss: 3.190 loss_x: 3.188 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.021 \n",
            "2023-09-10 19:31:59,637 - INFO - root -   Train Epoch: 1/  20. Iter:  141/1024. LR: 0.0300 batch_time: 4.753 data_time: 3.216 loss: 3.192 loss_x: 3.189 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.021 \n",
            "2023-09-10 19:32:02,703 - INFO - root -   Train Epoch: 1/  20. Iter:  142/1024. LR: 0.0300 batch_time: 4.741 data_time: 3.205 loss: 3.203 loss_x: 3.201 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.021 \n",
            "2023-09-10 19:32:06,385 - INFO - root -   Train Epoch: 1/  20. Iter:  143/1024. LR: 0.0300 batch_time: 4.733 data_time: 3.197 loss: 3.197 loss_x: 3.195 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.021 \n",
            "2023-09-10 19:32:09,862 - INFO - root -   Train Epoch: 1/  20. Iter:  144/1024. LR: 0.0300 batch_time: 4.725 data_time: 3.188 loss: 3.194 loss_x: 3.192 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.021 \n",
            "2023-09-10 19:32:12,666 - INFO - root -   Train Epoch: 1/  20. Iter:  145/1024. LR: 0.0300 batch_time: 4.711 data_time: 3.175 loss: 3.190 loss_x: 3.188 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.021 \n",
            "2023-09-10 19:32:15,717 - INFO - root -   Train Epoch: 1/  20. Iter:  146/1024. LR: 0.0300 batch_time: 4.700 data_time: 3.164 loss: 3.185 loss_x: 3.182 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.021 \n",
            "2023-09-10 19:32:18,635 - INFO - root -   Train Epoch: 1/  20. Iter:  147/1024. LR: 0.0300 batch_time: 4.688 data_time: 3.152 loss: 3.183 loss_x: 3.181 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.020 \n",
            "2023-09-10 19:32:22,605 - INFO - root -   Train Epoch: 1/  20. Iter:  148/1024. LR: 0.0300 batch_time: 4.683 data_time: 3.147 loss: 3.183 loss_x: 3.181 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.020 \n",
            "2023-09-10 19:32:25,677 - INFO - root -   Train Epoch: 1/  20. Iter:  149/1024. LR: 0.0300 batch_time: 4.672 data_time: 3.137 loss: 3.177 loss_x: 3.174 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.020 \n",
            "2023-09-10 19:32:28,406 - INFO - root -   Train Epoch: 1/  20. Iter:  150/1024. LR: 0.0300 batch_time: 4.659 data_time: 3.124 loss: 3.171 loss_x: 3.169 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.020 \n",
            "2023-09-10 19:32:31,484 - INFO - root -   Train Epoch: 1/  20. Iter:  151/1024. LR: 0.0300 batch_time: 4.649 data_time: 3.114 loss: 3.165 loss_x: 3.163 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.020 \n",
            "2023-09-10 19:32:35,149 - INFO - root -   Train Epoch: 1/  20. Iter:  152/1024. LR: 0.0300 batch_time: 4.642 data_time: 3.107 loss: 3.168 loss_x: 3.165 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.020 \n",
            "2023-09-10 19:32:38,975 - INFO - root -   Train Epoch: 1/  20. Iter:  153/1024. LR: 0.0300 batch_time: 4.637 data_time: 3.102 loss: 3.163 loss_x: 3.161 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.020 \n",
            "2023-09-10 19:32:41,762 - INFO - root -   Train Epoch: 1/  20. Iter:  154/1024. LR: 0.0300 batch_time: 4.625 data_time: 3.090 loss: 3.164 loss_x: 3.161 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.019 \n",
            "2023-09-10 19:32:44,424 - INFO - root -   Train Epoch: 1/  20. Iter:  155/1024. LR: 0.0300 batch_time: 4.612 data_time: 3.077 loss: 3.165 loss_x: 3.163 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.019 \n",
            "2023-09-10 19:32:47,515 - INFO - root -   Train Epoch: 1/  20. Iter:  156/1024. LR: 0.0300 batch_time: 4.603 data_time: 3.068 loss: 3.164 loss_x: 3.161 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.019 \n",
            "2023-09-10 19:32:51,741 - INFO - root -   Train Epoch: 1/  20. Iter:  157/1024. LR: 0.0300 batch_time: 4.600 data_time: 3.065 loss: 3.159 loss_x: 3.156 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.019 \n",
            "2023-09-10 19:32:54,822 - INFO - root -   Train Epoch: 1/  20. Iter:  158/1024. LR: 0.0300 batch_time: 4.591 data_time: 3.056 loss: 3.157 loss_x: 3.155 loss_u: 0.002 mask_prob: 0.005 pseudo_acc: 0.019 \n",
            "2023-09-10 19:32:57,664 - INFO - root -   Train Epoch: 1/  20. Iter:  159/1024. LR: 0.0300 batch_time: 4.580 data_time: 3.045 loss: 3.152 loss_x: 3.150 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.019 \n",
            "2023-09-10 19:33:00,512 - INFO - root -   Train Epoch: 1/  20. Iter:  160/1024. LR: 0.0300 batch_time: 4.569 data_time: 3.035 loss: 3.151 loss_x: 3.149 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.019 \n",
            "2023-09-10 19:33:03,606 - INFO - root -   Train Epoch: 1/  20. Iter:  161/1024. LR: 0.0300 batch_time: 4.560 data_time: 3.026 loss: 3.150 loss_x: 3.148 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.019 \n",
            "2023-09-10 19:33:07,633 - INFO - root -   Train Epoch: 1/  20. Iter:  162/1024. LR: 0.0300 batch_time: 4.556 data_time: 3.022 loss: 3.148 loss_x: 3.146 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.019 \n",
            "2023-09-10 19:33:10,328 - INFO - root -   Train Epoch: 1/  20. Iter:  163/1024. LR: 0.0300 batch_time: 4.545 data_time: 3.011 loss: 3.146 loss_x: 3.144 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:13,156 - INFO - root -   Train Epoch: 1/  20. Iter:  164/1024. LR: 0.0300 batch_time: 4.534 data_time: 3.001 loss: 3.144 loss_x: 3.142 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:15,963 - INFO - root -   Train Epoch: 1/  20. Iter:  165/1024. LR: 0.0300 batch_time: 4.524 data_time: 2.990 loss: 3.144 loss_x: 3.142 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:19,607 - INFO - root -   Train Epoch: 1/  20. Iter:  166/1024. LR: 0.0300 batch_time: 4.519 data_time: 2.985 loss: 3.150 loss_x: 3.148 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:23,198 - INFO - root -   Train Epoch: 1/  20. Iter:  167/1024. LR: 0.0300 batch_time: 4.513 data_time: 2.979 loss: 3.149 loss_x: 3.147 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:26,245 - INFO - root -   Train Epoch: 1/  20. Iter:  168/1024. LR: 0.0300 batch_time: 4.504 data_time: 2.971 loss: 3.144 loss_x: 3.142 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:29,071 - INFO - root -   Train Epoch: 1/  20. Iter:  169/1024. LR: 0.0300 batch_time: 4.494 data_time: 2.961 loss: 3.141 loss_x: 3.139 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:31,747 - INFO - root -   Train Epoch: 1/  20. Iter:  170/1024. LR: 0.0300 batch_time: 4.484 data_time: 2.950 loss: 3.141 loss_x: 3.138 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:36,116 - INFO - root -   Train Epoch: 1/  20. Iter:  171/1024. LR: 0.0300 batch_time: 4.483 data_time: 2.949 loss: 3.134 loss_x: 3.132 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.018 \n",
            "2023-09-10 19:33:39,049 - INFO - root -   Train Epoch: 1/  20. Iter:  172/1024. LR: 0.0300 batch_time: 4.474 data_time: 2.941 loss: 3.130 loss_x: 3.128 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:33:42,148 - INFO - root -   Train Epoch: 1/  20. Iter:  173/1024. LR: 0.0300 batch_time: 4.466 data_time: 2.933 loss: 3.127 loss_x: 3.125 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:33:44,949 - INFO - root -   Train Epoch: 1/  20. Iter:  174/1024. LR: 0.0300 batch_time: 4.457 data_time: 2.923 loss: 3.120 loss_x: 3.118 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:33:48,695 - INFO - root -   Train Epoch: 1/  20. Iter:  175/1024. LR: 0.0300 batch_time: 4.452 data_time: 2.919 loss: 3.134 loss_x: 3.132 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:33:52,476 - INFO - root -   Train Epoch: 1/  20. Iter:  176/1024. LR: 0.0300 batch_time: 4.449 data_time: 2.915 loss: 3.130 loss_x: 3.128 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:33:55,131 - INFO - root -   Train Epoch: 1/  20. Iter:  177/1024. LR: 0.0300 batch_time: 4.439 data_time: 2.905 loss: 3.130 loss_x: 3.128 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:33:58,047 - INFO - root -   Train Epoch: 1/  20. Iter:  178/1024. LR: 0.0300 batch_time: 4.430 data_time: 2.897 loss: 3.129 loss_x: 3.127 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:34:00,882 - INFO - root -   Train Epoch: 1/  20. Iter:  179/1024. LR: 0.0300 batch_time: 4.421 data_time: 2.888 loss: 3.128 loss_x: 3.126 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:34:05,231 - INFO - root -   Train Epoch: 1/  20. Iter:  180/1024. LR: 0.0300 batch_time: 4.421 data_time: 2.888 loss: 3.125 loss_x: 3.123 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:34:08,182 - INFO - root -   Train Epoch: 1/  20. Iter:  181/1024. LR: 0.0300 batch_time: 4.413 data_time: 2.880 loss: 3.124 loss_x: 3.122 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.017 \n",
            "2023-09-10 19:34:11,102 - INFO - root -   Train Epoch: 1/  20. Iter:  182/1024. LR: 0.0300 batch_time: 4.404 data_time: 2.872 loss: 3.121 loss_x: 3.119 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:13,950 - INFO - root -   Train Epoch: 1/  20. Iter:  183/1024. LR: 0.0300 batch_time: 4.396 data_time: 2.863 loss: 3.117 loss_x: 3.115 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:17,687 - INFO - root -   Train Epoch: 1/  20. Iter:  184/1024. LR: 0.0300 batch_time: 4.392 data_time: 2.860 loss: 3.116 loss_x: 3.114 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:21,438 - INFO - root -   Train Epoch: 1/  20. Iter:  185/1024. LR: 0.0300 batch_time: 4.389 data_time: 2.856 loss: 3.115 loss_x: 3.114 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:24,108 - INFO - root -   Train Epoch: 1/  20. Iter:  186/1024. LR: 0.0300 batch_time: 4.380 data_time: 2.847 loss: 3.114 loss_x: 3.112 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:27,202 - INFO - root -   Train Epoch: 1/  20. Iter:  187/1024. LR: 0.0300 batch_time: 4.373 data_time: 2.840 loss: 3.113 loss_x: 3.112 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:30,113 - INFO - root -   Train Epoch: 1/  20. Iter:  188/1024. LR: 0.0300 batch_time: 4.365 data_time: 2.833 loss: 3.111 loss_x: 3.109 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:33,681 - INFO - root -   Train Epoch: 1/  20. Iter:  189/1024. LR: 0.0300 batch_time: 4.361 data_time: 2.828 loss: 3.107 loss_x: 3.105 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:36,907 - INFO - root -   Train Epoch: 1/  20. Iter:  190/1024. LR: 0.0300 batch_time: 4.355 data_time: 2.822 loss: 3.105 loss_x: 3.103 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:39,897 - INFO - root -   Train Epoch: 1/  20. Iter:  191/1024. LR: 0.0300 batch_time: 4.348 data_time: 2.815 loss: 3.101 loss_x: 3.100 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:42,728 - INFO - root -   Train Epoch: 1/  20. Iter:  192/1024. LR: 0.0300 batch_time: 4.340 data_time: 2.808 loss: 3.102 loss_x: 3.100 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:45,438 - INFO - root -   Train Epoch: 1/  20. Iter:  193/1024. LR: 0.0300 batch_time: 4.331 data_time: 2.799 loss: 3.101 loss_x: 3.099 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.016 \n",
            "2023-09-10 19:34:49,369 - INFO - root -   Train Epoch: 1/  20. Iter:  194/1024. LR: 0.0300 batch_time: 4.329 data_time: 2.797 loss: 3.099 loss_x: 3.097 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:34:52,143 - INFO - root -   Train Epoch: 1/  20. Iter:  195/1024. LR: 0.0300 batch_time: 4.321 data_time: 2.789 loss: 3.100 loss_x: 3.098 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:34:54,839 - INFO - root -   Train Epoch: 1/  20. Iter:  196/1024. LR: 0.0300 batch_time: 4.313 data_time: 2.781 loss: 3.094 loss_x: 3.092 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:34:57,955 - INFO - root -   Train Epoch: 1/  20. Iter:  197/1024. LR: 0.0300 batch_time: 4.307 data_time: 2.775 loss: 3.093 loss_x: 3.091 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:01,918 - INFO - root -   Train Epoch: 1/  20. Iter:  198/1024. LR: 0.0300 batch_time: 4.305 data_time: 2.773 loss: 3.091 loss_x: 3.089 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:05,683 - INFO - root -   Train Epoch: 1/  20. Iter:  199/1024. LR: 0.0300 batch_time: 4.302 data_time: 2.771 loss: 3.089 loss_x: 3.088 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:08,768 - INFO - root -   Train Epoch: 1/  20. Iter:  200/1024. LR: 0.0300 batch_time: 4.296 data_time: 2.765 loss: 3.089 loss_x: 3.087 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:11,744 - INFO - root -   Train Epoch: 1/  20. Iter:  201/1024. LR: 0.0300 batch_time: 4.290 data_time: 2.758 loss: 3.091 loss_x: 3.089 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:14,710 - INFO - root -   Train Epoch: 1/  20. Iter:  202/1024. LR: 0.0300 batch_time: 4.283 data_time: 2.752 loss: 3.091 loss_x: 3.089 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:18,748 - INFO - root -   Train Epoch: 1/  20. Iter:  203/1024. LR: 0.0300 batch_time: 4.282 data_time: 2.751 loss: 3.089 loss_x: 3.087 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:21,460 - INFO - root -   Train Epoch: 1/  20. Iter:  204/1024. LR: 0.0300 batch_time: 4.274 data_time: 2.743 loss: 3.086 loss_x: 3.084 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:24,524 - INFO - root -   Train Epoch: 1/  20. Iter:  205/1024. LR: 0.0300 batch_time: 4.268 data_time: 2.737 loss: 3.087 loss_x: 3.085 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:27,651 - INFO - root -   Train Epoch: 1/  20. Iter:  206/1024. LR: 0.0300 batch_time: 4.263 data_time: 2.732 loss: 3.088 loss_x: 3.086 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.015 \n",
            "2023-09-10 19:35:31,483 - INFO - root -   Train Epoch: 1/  20. Iter:  207/1024. LR: 0.0300 batch_time: 4.261 data_time: 2.730 loss: 3.087 loss_x: 3.086 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:35:34,913 - INFO - root -   Train Epoch: 1/  20. Iter:  208/1024. LR: 0.0300 batch_time: 4.257 data_time: 2.726 loss: 3.085 loss_x: 3.083 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:35:37,880 - INFO - root -   Train Epoch: 1/  20. Iter:  209/1024. LR: 0.0300 batch_time: 4.251 data_time: 2.719 loss: 3.082 loss_x: 3.080 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:35:40,860 - INFO - root -   Train Epoch: 1/  20. Iter:  210/1024. LR: 0.0300 batch_time: 4.245 data_time: 2.714 loss: 3.082 loss_x: 3.080 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:35:43,918 - INFO - root -   Train Epoch: 1/  20. Iter:  211/1024. LR: 0.0300 batch_time: 4.239 data_time: 2.708 loss: 3.078 loss_x: 3.076 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:35:48,212 - INFO - root -   Train Epoch: 1/  20. Iter:  212/1024. LR: 0.0300 batch_time: 4.239 data_time: 2.708 loss: 3.077 loss_x: 3.075 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.014 \n",
            "2023-09-10 19:35:51,168 - INFO - root -   Train Epoch: 1/  20. Iter:  213/1024. LR: 0.0300 batch_time: 4.233 data_time: 2.702 loss: 3.072 loss_x: 3.070 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.014 \n",
            "2023-09-10 19:35:53,835 - INFO - root -   Train Epoch: 1/  20. Iter:  214/1024. LR: 0.0300 batch_time: 4.226 data_time: 2.695 loss: 3.069 loss_x: 3.067 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.014 \n",
            "2023-09-10 19:35:56,897 - INFO - root -   Train Epoch: 1/  20. Iter:  215/1024. LR: 0.0300 batch_time: 4.220 data_time: 2.690 loss: 3.065 loss_x: 3.063 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:36:01,127 - INFO - root -   Train Epoch: 1/  20. Iter:  216/1024. LR: 0.0300 batch_time: 4.220 data_time: 2.690 loss: 3.063 loss_x: 3.062 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:36:04,194 - INFO - root -   Train Epoch: 1/  20. Iter:  217/1024. LR: 0.0300 batch_time: 4.215 data_time: 2.684 loss: 3.065 loss_x: 3.063 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:36:07,366 - INFO - root -   Train Epoch: 1/  20. Iter:  218/1024. LR: 0.0300 batch_time: 4.210 data_time: 2.680 loss: 3.067 loss_x: 3.066 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:36:10,194 - INFO - root -   Train Epoch: 1/  20. Iter:  219/1024. LR: 0.0300 batch_time: 4.204 data_time: 2.674 loss: 3.066 loss_x: 3.064 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:36:13,325 - INFO - root -   Train Epoch: 1/  20. Iter:  220/1024. LR: 0.0300 batch_time: 4.199 data_time: 2.669 loss: 3.063 loss_x: 3.062 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:36:17,126 - INFO - root -   Train Epoch: 1/  20. Iter:  221/1024. LR: 0.0300 batch_time: 4.197 data_time: 2.667 loss: 3.061 loss_x: 3.059 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:36:20,265 - INFO - root -   Train Epoch: 1/  20. Iter:  222/1024. LR: 0.0300 batch_time: 4.193 data_time: 2.662 loss: 3.056 loss_x: 3.055 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.014 \n",
            "2023-09-10 19:36:23,191 - INFO - root -   Train Epoch: 1/  20. Iter:  223/1024. LR: 0.0300 batch_time: 4.187 data_time: 2.657 loss: 3.056 loss_x: 3.055 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.013 \n",
            "2023-09-10 19:36:26,126 - INFO - root -   Train Epoch: 1/  20. Iter:  224/1024. LR: 0.0300 batch_time: 4.181 data_time: 2.651 loss: 3.054 loss_x: 3.053 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.013 \n",
            "2023-09-10 19:36:30,553 - INFO - root -   Train Epoch: 1/  20. Iter:  225/1024. LR: 0.0300 batch_time: 4.182 data_time: 2.652 loss: 3.051 loss_x: 3.049 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.013 \n",
            "2023-09-10 19:36:33,533 - INFO - root -   Train Epoch: 1/  20. Iter:  226/1024. LR: 0.0300 batch_time: 4.177 data_time: 2.647 loss: 3.052 loss_x: 3.050 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.013 \n",
            "2023-09-10 19:36:36,554 - INFO - root -   Train Epoch: 1/  20. Iter:  227/1024. LR: 0.0300 batch_time: 4.172 data_time: 2.642 loss: 3.048 loss_x: 3.047 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.013 \n",
            "2023-09-10 19:36:39,252 - INFO - root -   Train Epoch: 1/  20. Iter:  228/1024. LR: 0.0300 batch_time: 4.166 data_time: 2.636 loss: 3.048 loss_x: 3.047 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.013 \n",
            "2023-09-10 19:36:42,504 - INFO - root -   Train Epoch: 1/  20. Iter:  229/1024. LR: 0.0300 batch_time: 4.162 data_time: 2.632 loss: 3.043 loss_x: 3.041 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.013 \n",
            "2023-09-10 19:36:46,370 - INFO - root -   Train Epoch: 1/  20. Iter:  230/1024. LR: 0.0300 batch_time: 4.160 data_time: 2.630 loss: 3.041 loss_x: 3.040 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.013 \n",
            "2023-09-10 19:36:49,315 - INFO - root -   Train Epoch: 1/  20. Iter:  231/1024. LR: 0.0300 batch_time: 4.155 data_time: 2.625 loss: 3.041 loss_x: 3.039 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:36:52,055 - INFO - root -   Train Epoch: 1/  20. Iter:  232/1024. LR: 0.0300 batch_time: 4.149 data_time: 2.619 loss: 3.038 loss_x: 3.037 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:36:54,724 - INFO - root -   Train Epoch: 1/  20. Iter:  233/1024. LR: 0.0300 batch_time: 4.143 data_time: 2.613 loss: 3.035 loss_x: 3.033 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:36:58,630 - INFO - root -   Train Epoch: 1/  20. Iter:  234/1024. LR: 0.0300 batch_time: 4.142 data_time: 2.612 loss: 3.037 loss_x: 3.035 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:37:02,116 - INFO - root -   Train Epoch: 1/  20. Iter:  235/1024. LR: 0.0300 batch_time: 4.139 data_time: 2.609 loss: 3.031 loss_x: 3.029 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:37:05,215 - INFO - root -   Train Epoch: 1/  20. Iter:  236/1024. LR: 0.0300 batch_time: 4.134 data_time: 2.605 loss: 3.029 loss_x: 3.028 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:37:08,123 - INFO - root -   Train Epoch: 1/  20. Iter:  237/1024. LR: 0.0300 batch_time: 4.129 data_time: 2.600 loss: 3.024 loss_x: 3.022 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.021 \n",
            "2023-09-10 19:37:11,193 - INFO - root -   Train Epoch: 1/  20. Iter:  238/1024. LR: 0.0300 batch_time: 4.125 data_time: 2.595 loss: 3.019 loss_x: 3.017 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:37:15,391 - INFO - root -   Train Epoch: 1/  20. Iter:  239/1024. LR: 0.0300 batch_time: 4.125 data_time: 2.596 loss: 3.019 loss_x: 3.017 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:37:18,443 - INFO - root -   Train Epoch: 1/  20. Iter:  240/1024. LR: 0.0300 batch_time: 4.121 data_time: 2.591 loss: 3.016 loss_x: 3.015 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:37:21,219 - INFO - root -   Train Epoch: 1/  20. Iter:  241/1024. LR: 0.0300 batch_time: 4.115 data_time: 2.586 loss: 3.015 loss_x: 3.013 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:37:24,032 - INFO - root -   Train Epoch: 1/  20. Iter:  242/1024. LR: 0.0300 batch_time: 4.110 data_time: 2.580 loss: 3.018 loss_x: 3.016 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:37:27,987 - INFO - root -   Train Epoch: 1/  20. Iter:  243/1024. LR: 0.0300 batch_time: 4.109 data_time: 2.580 loss: 3.020 loss_x: 3.018 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:37:31,597 - INFO - root -   Train Epoch: 1/  20. Iter:  244/1024. LR: 0.0300 batch_time: 4.107 data_time: 2.578 loss: 3.017 loss_x: 3.015 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.025 \n",
            "2023-09-10 19:37:34,690 - INFO - root -   Train Epoch: 1/  20. Iter:  245/1024. LR: 0.0300 batch_time: 4.103 data_time: 2.573 loss: 3.018 loss_x: 3.016 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:37:37,591 - INFO - root -   Train Epoch: 1/  20. Iter:  246/1024. LR: 0.0300 batch_time: 4.098 data_time: 2.569 loss: 3.020 loss_x: 3.018 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:37:40,728 - INFO - root -   Train Epoch: 1/  20. Iter:  247/1024. LR: 0.0300 batch_time: 4.094 data_time: 2.565 loss: 3.018 loss_x: 3.016 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:37:44,951 - INFO - root -   Train Epoch: 1/  20. Iter:  248/1024. LR: 0.0300 batch_time: 4.095 data_time: 2.565 loss: 3.018 loss_x: 3.016 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:37:48,026 - INFO - root -   Train Epoch: 1/  20. Iter:  249/1024. LR: 0.0300 batch_time: 4.090 data_time: 2.561 loss: 3.019 loss_x: 3.017 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:37:50,994 - INFO - root -   Train Epoch: 1/  20. Iter:  250/1024. LR: 0.0300 batch_time: 4.086 data_time: 2.557 loss: 3.023 loss_x: 3.020 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:37:53,788 - INFO - root -   Train Epoch: 1/  20. Iter:  251/1024. LR: 0.0300 batch_time: 4.081 data_time: 2.552 loss: 3.022 loss_x: 3.020 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:37:57,821 - INFO - root -   Train Epoch: 1/  20. Iter:  252/1024. LR: 0.0300 batch_time: 4.081 data_time: 2.551 loss: 3.021 loss_x: 3.019 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:38:00,793 - INFO - root -   Train Epoch: 1/  20. Iter:  253/1024. LR: 0.0300 batch_time: 4.076 data_time: 2.547 loss: 3.022 loss_x: 3.019 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:38:03,940 - INFO - root -   Train Epoch: 1/  20. Iter:  254/1024. LR: 0.0300 batch_time: 4.073 data_time: 2.544 loss: 3.019 loss_x: 3.017 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:38:06,663 - INFO - root -   Train Epoch: 1/  20. Iter:  255/1024. LR: 0.0300 batch_time: 4.067 data_time: 2.538 loss: 3.017 loss_x: 3.015 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.024 \n",
            "2023-09-10 19:38:09,575 - INFO - root -   Train Epoch: 1/  20. Iter:  256/1024. LR: 0.0300 batch_time: 4.063 data_time: 2.534 loss: 3.014 loss_x: 3.012 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:13,792 - INFO - root -   Train Epoch: 1/  20. Iter:  257/1024. LR: 0.0300 batch_time: 4.063 data_time: 2.534 loss: 3.015 loss_x: 3.013 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:16,725 - INFO - root -   Train Epoch: 1/  20. Iter:  258/1024. LR: 0.0300 batch_time: 4.059 data_time: 2.530 loss: 3.014 loss_x: 3.012 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:19,691 - INFO - root -   Train Epoch: 1/  20. Iter:  259/1024. LR: 0.0300 batch_time: 4.055 data_time: 2.526 loss: 3.012 loss_x: 3.010 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:22,479 - INFO - root -   Train Epoch: 1/  20. Iter:  260/1024. LR: 0.0300 batch_time: 4.050 data_time: 2.521 loss: 3.009 loss_x: 3.007 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:26,471 - INFO - root -   Train Epoch: 1/  20. Iter:  261/1024. LR: 0.0300 batch_time: 4.050 data_time: 2.521 loss: 3.009 loss_x: 3.007 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:30,066 - INFO - root -   Train Epoch: 1/  20. Iter:  262/1024. LR: 0.0300 batch_time: 4.048 data_time: 2.519 loss: 3.008 loss_x: 3.006 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:33,014 - INFO - root -   Train Epoch: 1/  20. Iter:  263/1024. LR: 0.0300 batch_time: 4.044 data_time: 2.515 loss: 3.007 loss_x: 3.005 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:36,137 - INFO - root -   Train Epoch: 1/  20. Iter:  264/1024. LR: 0.0300 batch_time: 4.040 data_time: 2.512 loss: 3.007 loss_x: 3.005 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:39,432 - INFO - root -   Train Epoch: 1/  20. Iter:  265/1024. LR: 0.0300 batch_time: 4.037 data_time: 2.509 loss: 3.005 loss_x: 3.003 loss_u: 0.002 mask_prob: 0.004 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:43,321 - INFO - root -   Train Epoch: 1/  20. Iter:  266/1024. LR: 0.0300 batch_time: 4.037 data_time: 2.508 loss: 3.003 loss_x: 3.001 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.023 \n",
            "2023-09-10 19:38:46,427 - INFO - root -   Train Epoch: 1/  20. Iter:  267/1024. LR: 0.0300 batch_time: 4.033 data_time: 2.505 loss: 3.001 loss_x: 2.999 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:38:49,269 - INFO - root -   Train Epoch: 1/  20. Iter:  268/1024. LR: 0.0300 batch_time: 4.029 data_time: 2.500 loss: 2.998 loss_x: 2.996 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:38:51,978 - INFO - root -   Train Epoch: 1/  20. Iter:  269/1024. LR: 0.0300 batch_time: 4.024 data_time: 2.496 loss: 2.997 loss_x: 2.995 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:38:55,462 - INFO - root -   Train Epoch: 1/  20. Iter:  270/1024. LR: 0.0300 batch_time: 4.022 data_time: 2.494 loss: 2.996 loss_x: 2.994 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:38:59,209 - INFO - root -   Train Epoch: 1/  20. Iter:  271/1024. LR: 0.0300 batch_time: 4.021 data_time: 2.492 loss: 2.995 loss_x: 2.993 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:01,900 - INFO - root -   Train Epoch: 1/  20. Iter:  272/1024. LR: 0.0300 batch_time: 4.016 data_time: 2.488 loss: 2.996 loss_x: 2.994 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:04,681 - INFO - root -   Train Epoch: 1/  20. Iter:  273/1024. LR: 0.0300 batch_time: 4.012 data_time: 2.483 loss: 3.001 loss_x: 2.999 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:07,736 - INFO - root -   Train Epoch: 1/  20. Iter:  274/1024. LR: 0.0300 batch_time: 4.008 data_time: 2.480 loss: 2.998 loss_x: 2.996 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:11,350 - INFO - root -   Train Epoch: 1/  20. Iter:  275/1024. LR: 0.0300 batch_time: 4.007 data_time: 2.478 loss: 2.999 loss_x: 2.997 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:14,128 - INFO - root -   Train Epoch: 1/  20. Iter:  276/1024. LR: 0.0300 batch_time: 4.002 data_time: 2.474 loss: 3.003 loss_x: 3.001 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:17,087 - INFO - root -   Train Epoch: 1/  20. Iter:  277/1024. LR: 0.0300 batch_time: 3.998 data_time: 2.470 loss: 3.003 loss_x: 3.001 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:19,931 - INFO - root -   Train Epoch: 1/  20. Iter:  278/1024. LR: 0.0300 batch_time: 3.994 data_time: 2.466 loss: 3.003 loss_x: 3.001 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:23,005 - INFO - root -   Train Epoch: 1/  20. Iter:  279/1024. LR: 0.0300 batch_time: 3.991 data_time: 2.463 loss: 3.004 loss_x: 3.002 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.022 \n",
            "2023-09-10 19:39:26,549 - INFO - root -   Train Epoch: 1/  20. Iter:  280/1024. LR: 0.0300 batch_time: 3.989 data_time: 2.461 loss: 3.003 loss_x: 3.002 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:29,368 - INFO - root -   Train Epoch: 1/  20. Iter:  281/1024. LR: 0.0300 batch_time: 3.985 data_time: 2.457 loss: 3.002 loss_x: 3.000 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:32,430 - INFO - root -   Train Epoch: 1/  20. Iter:  282/1024. LR: 0.0300 batch_time: 3.982 data_time: 2.454 loss: 2.998 loss_x: 2.996 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:35,243 - INFO - root -   Train Epoch: 1/  20. Iter:  283/1024. LR: 0.0300 batch_time: 3.978 data_time: 2.450 loss: 2.995 loss_x: 2.993 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:38,787 - INFO - root -   Train Epoch: 1/  20. Iter:  284/1024. LR: 0.0300 batch_time: 3.976 data_time: 2.448 loss: 2.994 loss_x: 2.992 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:42,503 - INFO - root -   Train Epoch: 1/  20. Iter:  285/1024. LR: 0.0300 batch_time: 3.975 data_time: 2.447 loss: 2.992 loss_x: 2.990 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:45,555 - INFO - root -   Train Epoch: 1/  20. Iter:  286/1024. LR: 0.0300 batch_time: 3.972 data_time: 2.444 loss: 2.989 loss_x: 2.987 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:48,445 - INFO - root -   Train Epoch: 1/  20. Iter:  287/1024. LR: 0.0300 batch_time: 3.968 data_time: 2.440 loss: 2.989 loss_x: 2.988 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:51,395 - INFO - root -   Train Epoch: 1/  20. Iter:  288/1024. LR: 0.0300 batch_time: 3.965 data_time: 2.437 loss: 2.987 loss_x: 2.986 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:55,454 - INFO - root -   Train Epoch: 1/  20. Iter:  289/1024. LR: 0.0300 batch_time: 3.965 data_time: 2.437 loss: 2.984 loss_x: 2.983 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:39:58,149 - INFO - root -   Train Epoch: 1/  20. Iter:  290/1024. LR: 0.0300 batch_time: 3.961 data_time: 2.433 loss: 2.985 loss_x: 2.983 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:40:00,893 - INFO - root -   Train Epoch: 1/  20. Iter:  291/1024. LR: 0.0300 batch_time: 3.957 data_time: 2.429 loss: 2.982 loss_x: 2.980 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:40:03,580 - INFO - root -   Train Epoch: 1/  20. Iter:  292/1024. LR: 0.0300 batch_time: 3.952 data_time: 2.424 loss: 2.980 loss_x: 2.978 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.021 \n",
            "2023-09-10 19:40:06,443 - INFO - root -   Train Epoch: 1/  20. Iter:  293/1024. LR: 0.0300 batch_time: 3.949 data_time: 2.421 loss: 2.977 loss_x: 2.976 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:10,317 - INFO - root -   Train Epoch: 1/  20. Iter:  294/1024. LR: 0.0300 batch_time: 3.948 data_time: 2.420 loss: 2.980 loss_x: 2.979 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:13,406 - INFO - root -   Train Epoch: 1/  20. Iter:  295/1024. LR: 0.0300 batch_time: 3.945 data_time: 2.417 loss: 2.978 loss_x: 2.976 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:16,273 - INFO - root -   Train Epoch: 1/  20. Iter:  296/1024. LR: 0.0300 batch_time: 3.942 data_time: 2.414 loss: 2.974 loss_x: 2.972 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:18,957 - INFO - root -   Train Epoch: 1/  20. Iter:  297/1024. LR: 0.0300 batch_time: 3.938 data_time: 2.410 loss: 2.972 loss_x: 2.971 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:22,699 - INFO - root -   Train Epoch: 1/  20. Iter:  298/1024. LR: 0.0300 batch_time: 3.937 data_time: 2.409 loss: 2.969 loss_x: 2.968 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:26,213 - INFO - root -   Train Epoch: 1/  20. Iter:  299/1024. LR: 0.0300 batch_time: 3.935 data_time: 2.408 loss: 2.968 loss_x: 2.966 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:29,169 - INFO - root -   Train Epoch: 1/  20. Iter:  300/1024. LR: 0.0300 batch_time: 3.932 data_time: 2.404 loss: 2.969 loss_x: 2.967 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:32,280 - INFO - root -   Train Epoch: 1/  20. Iter:  301/1024. LR: 0.0300 batch_time: 3.929 data_time: 2.402 loss: 2.967 loss_x: 2.965 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:35,167 - INFO - root -   Train Epoch: 1/  20. Iter:  302/1024. LR: 0.0300 batch_time: 3.926 data_time: 2.398 loss: 2.968 loss_x: 2.966 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:39,738 - INFO - root -   Train Epoch: 1/  20. Iter:  303/1024. LR: 0.0300 batch_time: 3.928 data_time: 2.400 loss: 2.967 loss_x: 2.965 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:42,902 - INFO - root -   Train Epoch: 1/  20. Iter:  304/1024. LR: 0.0300 batch_time: 3.926 data_time: 2.398 loss: 2.969 loss_x: 2.967 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:45,928 - INFO - root -   Train Epoch: 1/  20. Iter:  305/1024. LR: 0.0300 batch_time: 3.923 data_time: 2.395 loss: 2.970 loss_x: 2.968 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:48,967 - INFO - root -   Train Epoch: 1/  20. Iter:  306/1024. LR: 0.0300 batch_time: 3.920 data_time: 2.392 loss: 2.972 loss_x: 2.970 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:52,261 - INFO - root -   Train Epoch: 1/  20. Iter:  307/1024. LR: 0.0300 batch_time: 3.918 data_time: 2.390 loss: 2.969 loss_x: 2.967 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:40:55,960 - INFO - root -   Train Epoch: 1/  20. Iter:  308/1024. LR: 0.0300 batch_time: 3.917 data_time: 2.389 loss: 2.969 loss_x: 2.967 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:40:59,070 - INFO - root -   Train Epoch: 1/  20. Iter:  309/1024. LR: 0.0300 batch_time: 3.914 data_time: 2.387 loss: 2.968 loss_x: 2.966 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:01,919 - INFO - root -   Train Epoch: 1/  20. Iter:  310/1024. LR: 0.0300 batch_time: 3.911 data_time: 2.384 loss: 2.969 loss_x: 2.968 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:05,022 - INFO - root -   Train Epoch: 1/  20. Iter:  311/1024. LR: 0.0300 batch_time: 3.908 data_time: 2.381 loss: 2.967 loss_x: 2.965 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:08,591 - INFO - root -   Train Epoch: 1/  20. Iter:  312/1024. LR: 0.0300 batch_time: 3.907 data_time: 2.380 loss: 2.967 loss_x: 2.965 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:11,440 - INFO - root -   Train Epoch: 1/  20. Iter:  313/1024. LR: 0.0300 batch_time: 3.904 data_time: 2.377 loss: 2.967 loss_x: 2.965 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:14,502 - INFO - root -   Train Epoch: 1/  20. Iter:  314/1024. LR: 0.0300 batch_time: 3.901 data_time: 2.374 loss: 2.965 loss_x: 2.963 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:17,575 - INFO - root -   Train Epoch: 1/  20. Iter:  315/1024. LR: 0.0300 batch_time: 3.899 data_time: 2.371 loss: 2.960 loss_x: 2.959 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:21,034 - INFO - root -   Train Epoch: 1/  20. Iter:  316/1024. LR: 0.0300 batch_time: 3.897 data_time: 2.370 loss: 2.959 loss_x: 2.958 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:25,137 - INFO - root -   Train Epoch: 1/  20. Iter:  317/1024. LR: 0.0300 batch_time: 3.898 data_time: 2.370 loss: 2.957 loss_x: 2.955 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:27,922 - INFO - root -   Train Epoch: 1/  20. Iter:  318/1024. LR: 0.0300 batch_time: 3.894 data_time: 2.367 loss: 2.954 loss_x: 2.952 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:30,653 - INFO - root -   Train Epoch: 1/  20. Iter:  319/1024. LR: 0.0300 batch_time: 3.891 data_time: 2.363 loss: 2.953 loss_x: 2.951 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:33,663 - INFO - root -   Train Epoch: 1/  20. Iter:  320/1024. LR: 0.0300 batch_time: 3.888 data_time: 2.361 loss: 2.952 loss_x: 2.951 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:37,331 - INFO - root -   Train Epoch: 1/  20. Iter:  321/1024. LR: 0.0300 batch_time: 3.887 data_time: 2.360 loss: 2.950 loss_x: 2.948 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:40,760 - INFO - root -   Train Epoch: 1/  20. Iter:  322/1024. LR: 0.0300 batch_time: 3.886 data_time: 2.358 loss: 2.947 loss_x: 2.946 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:43,714 - INFO - root -   Train Epoch: 1/  20. Iter:  323/1024. LR: 0.0300 batch_time: 3.883 data_time: 2.356 loss: 2.947 loss_x: 2.945 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:46,542 - INFO - root -   Train Epoch: 1/  20. Iter:  324/1024. LR: 0.0300 batch_time: 3.880 data_time: 2.352 loss: 2.946 loss_x: 2.944 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:41:49,710 - INFO - root -   Train Epoch: 1/  20. Iter:  325/1024. LR: 0.0300 batch_time: 3.878 data_time: 2.350 loss: 2.946 loss_x: 2.944 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:41:54,050 - INFO - root -   Train Epoch: 1/  20. Iter:  326/1024. LR: 0.0300 batch_time: 3.879 data_time: 2.352 loss: 2.947 loss_x: 2.945 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:41:57,172 - INFO - root -   Train Epoch: 1/  20. Iter:  327/1024. LR: 0.0300 batch_time: 3.877 data_time: 2.349 loss: 2.947 loss_x: 2.945 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:00,258 - INFO - root -   Train Epoch: 1/  20. Iter:  328/1024. LR: 0.0300 batch_time: 3.874 data_time: 2.347 loss: 2.944 loss_x: 2.942 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:03,195 - INFO - root -   Train Epoch: 1/  20. Iter:  329/1024. LR: 0.0300 batch_time: 3.871 data_time: 2.344 loss: 2.943 loss_x: 2.942 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:07,535 - INFO - root -   Train Epoch: 1/  20. Iter:  330/1024. LR: 0.0300 batch_time: 3.873 data_time: 2.346 loss: 2.942 loss_x: 2.940 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:10,645 - INFO - root -   Train Epoch: 1/  20. Iter:  331/1024. LR: 0.0300 batch_time: 3.870 data_time: 2.343 loss: 2.940 loss_x: 2.939 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:13,474 - INFO - root -   Train Epoch: 1/  20. Iter:  332/1024. LR: 0.0300 batch_time: 3.867 data_time: 2.340 loss: 2.939 loss_x: 2.937 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:16,351 - INFO - root -   Train Epoch: 1/  20. Iter:  333/1024. LR: 0.0300 batch_time: 3.864 data_time: 2.337 loss: 2.938 loss_x: 2.937 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:19,861 - INFO - root -   Train Epoch: 1/  20. Iter:  334/1024. LR: 0.0300 batch_time: 3.863 data_time: 2.336 loss: 2.937 loss_x: 2.935 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:24,059 - INFO - root -   Train Epoch: 1/  20. Iter:  335/1024. LR: 0.0300 batch_time: 3.864 data_time: 2.337 loss: 2.937 loss_x: 2.935 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:27,144 - INFO - root -   Train Epoch: 1/  20. Iter:  336/1024. LR: 0.0300 batch_time: 3.862 data_time: 2.335 loss: 2.935 loss_x: 2.933 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:30,031 - INFO - root -   Train Epoch: 1/  20. Iter:  337/1024. LR: 0.0300 batch_time: 3.859 data_time: 2.332 loss: 2.934 loss_x: 2.933 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:33,182 - INFO - root -   Train Epoch: 1/  20. Iter:  338/1024. LR: 0.0300 batch_time: 3.857 data_time: 2.330 loss: 2.933 loss_x: 2.931 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:37,597 - INFO - root -   Train Epoch: 1/  20. Iter:  339/1024. LR: 0.0300 batch_time: 3.859 data_time: 2.331 loss: 2.933 loss_x: 2.931 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:40,512 - INFO - root -   Train Epoch: 1/  20. Iter:  340/1024. LR: 0.0300 batch_time: 3.856 data_time: 2.329 loss: 2.932 loss_x: 2.930 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:43,392 - INFO - root -   Train Epoch: 1/  20. Iter:  341/1024. LR: 0.0300 batch_time: 3.853 data_time: 2.326 loss: 2.930 loss_x: 2.929 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:46,276 - INFO - root -   Train Epoch: 1/  20. Iter:  342/1024. LR: 0.0300 batch_time: 3.850 data_time: 2.323 loss: 2.928 loss_x: 2.927 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:42:50,228 - INFO - root -   Train Epoch: 1/  20. Iter:  343/1024. LR: 0.0300 batch_time: 3.850 data_time: 2.323 loss: 2.928 loss_x: 2.927 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:42:54,076 - INFO - root -   Train Epoch: 1/  20. Iter:  344/1024. LR: 0.0300 batch_time: 3.850 data_time: 2.323 loss: 2.926 loss_x: 2.924 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:42:57,057 - INFO - root -   Train Epoch: 1/  20. Iter:  345/1024. LR: 0.0300 batch_time: 3.848 data_time: 2.321 loss: 2.921 loss_x: 2.919 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:43:00,156 - INFO - root -   Train Epoch: 1/  20. Iter:  346/1024. LR: 0.0300 batch_time: 3.846 data_time: 2.319 loss: 2.919 loss_x: 2.918 loss_u: 0.002 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:43:03,007 - INFO - root -   Train Epoch: 1/  20. Iter:  347/1024. LR: 0.0300 batch_time: 3.843 data_time: 2.316 loss: 2.916 loss_x: 2.915 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:43:07,132 - INFO - root -   Train Epoch: 1/  20. Iter:  348/1024. LR: 0.0300 batch_time: 3.844 data_time: 2.317 loss: 2.914 loss_x: 2.912 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:43:10,152 - INFO - root -   Train Epoch: 1/  20. Iter:  349/1024. LR: 0.0300 batch_time: 3.841 data_time: 2.314 loss: 2.912 loss_x: 2.910 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:43:13,015 - INFO - root -   Train Epoch: 1/  20. Iter:  350/1024. LR: 0.0300 batch_time: 3.839 data_time: 2.312 loss: 2.908 loss_x: 2.907 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.017 \n",
            "2023-09-10 19:43:16,090 - INFO - root -   Train Epoch: 1/  20. Iter:  351/1024. LR: 0.0300 batch_time: 3.836 data_time: 2.309 loss: 2.907 loss_x: 2.905 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:43:19,857 - INFO - root -   Train Epoch: 1/  20. Iter:  352/1024. LR: 0.0300 batch_time: 3.836 data_time: 2.309 loss: 2.904 loss_x: 2.903 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:43:23,060 - INFO - root -   Train Epoch: 1/  20. Iter:  353/1024. LR: 0.0300 batch_time: 3.834 data_time: 2.307 loss: 2.904 loss_x: 2.902 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:43:25,835 - INFO - root -   Train Epoch: 1/  20. Iter:  354/1024. LR: 0.0300 batch_time: 3.831 data_time: 2.304 loss: 2.903 loss_x: 2.901 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:43:28,657 - INFO - root -   Train Epoch: 1/  20. Iter:  355/1024. LR: 0.0300 batch_time: 3.829 data_time: 2.302 loss: 2.902 loss_x: 2.901 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:43:31,755 - INFO - root -   Train Epoch: 1/  20. Iter:  356/1024. LR: 0.0300 batch_time: 3.827 data_time: 2.300 loss: 2.903 loss_x: 2.902 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:43:35,597 - INFO - root -   Train Epoch: 1/  20. Iter:  357/1024. LR: 0.0300 batch_time: 3.827 data_time: 2.300 loss: 2.903 loss_x: 2.902 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:43:38,655 - INFO - root -   Train Epoch: 1/  20. Iter:  358/1024. LR: 0.0300 batch_time: 3.824 data_time: 2.297 loss: 2.904 loss_x: 2.902 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.020 \n",
            "2023-09-10 19:43:41,717 - INFO - root -   Train Epoch: 1/  20. Iter:  359/1024. LR: 0.0300 batch_time: 3.822 data_time: 2.295 loss: 2.902 loss_x: 2.901 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:43:44,507 - INFO - root -   Train Epoch: 1/  20. Iter:  360/1024. LR: 0.0300 batch_time: 3.819 data_time: 2.293 loss: 2.899 loss_x: 2.898 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:43:48,410 - INFO - root -   Train Epoch: 1/  20. Iter:  361/1024. LR: 0.0300 batch_time: 3.820 data_time: 2.293 loss: 2.899 loss_x: 2.898 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:43:52,056 - INFO - root -   Train Epoch: 1/  20. Iter:  362/1024. LR: 0.0300 batch_time: 3.819 data_time: 2.292 loss: 2.897 loss_x: 2.896 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:43:54,863 - INFO - root -   Train Epoch: 1/  20. Iter:  363/1024. LR: 0.0300 batch_time: 3.816 data_time: 2.290 loss: 2.896 loss_x: 2.894 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:43:57,569 - INFO - root -   Train Epoch: 1/  20. Iter:  364/1024. LR: 0.0300 batch_time: 3.813 data_time: 2.287 loss: 2.893 loss_x: 2.892 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:00,258 - INFO - root -   Train Epoch: 1/  20. Iter:  365/1024. LR: 0.0300 batch_time: 3.810 data_time: 2.283 loss: 2.892 loss_x: 2.891 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:04,463 - INFO - root -   Train Epoch: 1/  20. Iter:  366/1024. LR: 0.0300 batch_time: 3.811 data_time: 2.284 loss: 2.897 loss_x: 2.895 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:07,487 - INFO - root -   Train Epoch: 1/  20. Iter:  367/1024. LR: 0.0300 batch_time: 3.809 data_time: 2.282 loss: 2.897 loss_x: 2.895 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:10,493 - INFO - root -   Train Epoch: 1/  20. Iter:  368/1024. LR: 0.0300 batch_time: 3.807 data_time: 2.280 loss: 2.896 loss_x: 2.895 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:13,577 - INFO - root -   Train Epoch: 1/  20. Iter:  369/1024. LR: 0.0300 batch_time: 3.805 data_time: 2.278 loss: 2.896 loss_x: 2.895 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:16,417 - INFO - root -   Train Epoch: 1/  20. Iter:  370/1024. LR: 0.0300 batch_time: 3.802 data_time: 2.276 loss: 2.895 loss_x: 2.894 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:20,173 - INFO - root -   Train Epoch: 1/  20. Iter:  371/1024. LR: 0.0300 batch_time: 3.802 data_time: 2.276 loss: 2.894 loss_x: 2.893 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:23,381 - INFO - root -   Train Epoch: 1/  20. Iter:  372/1024. LR: 0.0300 batch_time: 3.801 data_time: 2.274 loss: 2.897 loss_x: 2.895 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:26,248 - INFO - root -   Train Epoch: 1/  20. Iter:  373/1024. LR: 0.0300 batch_time: 3.798 data_time: 2.272 loss: 2.895 loss_x: 2.893 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:29,376 - INFO - root -   Train Epoch: 1/  20. Iter:  374/1024. LR: 0.0300 batch_time: 3.796 data_time: 2.270 loss: 2.895 loss_x: 2.893 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:33,104 - INFO - root -   Train Epoch: 1/  20. Iter:  375/1024. LR: 0.0300 batch_time: 3.796 data_time: 2.270 loss: 2.892 loss_x: 2.890 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:36,563 - INFO - root -   Train Epoch: 1/  20. Iter:  376/1024. LR: 0.0300 batch_time: 3.795 data_time: 2.269 loss: 2.887 loss_x: 2.886 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:39,630 - INFO - root -   Train Epoch: 1/  20. Iter:  377/1024. LR: 0.0300 batch_time: 3.793 data_time: 2.267 loss: 2.887 loss_x: 2.886 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:42,680 - INFO - root -   Train Epoch: 1/  20. Iter:  378/1024. LR: 0.0300 batch_time: 3.791 data_time: 2.265 loss: 2.885 loss_x: 2.884 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.019 \n",
            "2023-09-10 19:44:45,575 - INFO - root -   Train Epoch: 1/  20. Iter:  379/1024. LR: 0.0300 batch_time: 3.789 data_time: 2.263 loss: 2.884 loss_x: 2.883 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:44:49,795 - INFO - root -   Train Epoch: 1/  20. Iter:  380/1024. LR: 0.0300 batch_time: 3.790 data_time: 2.264 loss: 2.883 loss_x: 2.881 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:44:52,870 - INFO - root -   Train Epoch: 1/  20. Iter:  381/1024. LR: 0.0300 batch_time: 3.788 data_time: 2.262 loss: 2.882 loss_x: 2.881 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:44:55,825 - INFO - root -   Train Epoch: 1/  20. Iter:  382/1024. LR: 0.0300 batch_time: 3.786 data_time: 2.260 loss: 2.878 loss_x: 2.877 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:44:58,590 - INFO - root -   Train Epoch: 1/  20. Iter:  383/1024. LR: 0.0300 batch_time: 3.783 data_time: 2.257 loss: 2.878 loss_x: 2.877 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:02,317 - INFO - root -   Train Epoch: 1/  20. Iter:  384/1024. LR: 0.0300 batch_time: 3.783 data_time: 2.257 loss: 2.879 loss_x: 2.877 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:05,486 - INFO - root -   Train Epoch: 1/  20. Iter:  385/1024. LR: 0.0300 batch_time: 3.782 data_time: 2.255 loss: 2.880 loss_x: 2.879 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:08,184 - INFO - root -   Train Epoch: 1/  20. Iter:  386/1024. LR: 0.0300 batch_time: 3.779 data_time: 2.253 loss: 2.880 loss_x: 2.878 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:10,931 - INFO - root -   Train Epoch: 1/  20. Iter:  387/1024. LR: 0.0300 batch_time: 3.776 data_time: 2.250 loss: 2.878 loss_x: 2.877 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:13,944 - INFO - root -   Train Epoch: 1/  20. Iter:  388/1024. LR: 0.0300 batch_time: 3.774 data_time: 2.248 loss: 2.875 loss_x: 2.874 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:18,460 - INFO - root -   Train Epoch: 1/  20. Iter:  389/1024. LR: 0.0300 batch_time: 3.776 data_time: 2.250 loss: 2.875 loss_x: 2.873 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:21,349 - INFO - root -   Train Epoch: 1/  20. Iter:  390/1024. LR: 0.0300 batch_time: 3.774 data_time: 2.248 loss: 2.875 loss_x: 2.874 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:24,189 - INFO - root -   Train Epoch: 1/  20. Iter:  391/1024. LR: 0.0300 batch_time: 3.772 data_time: 2.245 loss: 2.874 loss_x: 2.872 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:26,962 - INFO - root -   Train Epoch: 1/  20. Iter:  392/1024. LR: 0.0300 batch_time: 3.769 data_time: 2.243 loss: 2.874 loss_x: 2.873 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:29,696 - INFO - root -   Train Epoch: 1/  20. Iter:  393/1024. LR: 0.0300 batch_time: 3.766 data_time: 2.240 loss: 2.871 loss_x: 2.870 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:33,496 - INFO - root -   Train Epoch: 1/  20. Iter:  394/1024. LR: 0.0300 batch_time: 3.766 data_time: 2.240 loss: 2.871 loss_x: 2.870 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:36,403 - INFO - root -   Train Epoch: 1/  20. Iter:  395/1024. LR: 0.0300 batch_time: 3.764 data_time: 2.238 loss: 2.869 loss_x: 2.868 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:39,588 - INFO - root -   Train Epoch: 1/  20. Iter:  396/1024. LR: 0.0300 batch_time: 3.763 data_time: 2.237 loss: 2.869 loss_x: 2.867 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:42,273 - INFO - root -   Train Epoch: 1/  20. Iter:  397/1024. LR: 0.0300 batch_time: 3.760 data_time: 2.234 loss: 2.867 loss_x: 2.865 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:46,321 - INFO - root -   Train Epoch: 1/  20. Iter:  398/1024. LR: 0.0300 batch_time: 3.761 data_time: 2.235 loss: 2.867 loss_x: 2.865 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:49,725 - INFO - root -   Train Epoch: 1/  20. Iter:  399/1024. LR: 0.0300 batch_time: 3.760 data_time: 2.234 loss: 2.866 loss_x: 2.865 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:52,772 - INFO - root -   Train Epoch: 1/  20. Iter:  400/1024. LR: 0.0300 batch_time: 3.758 data_time: 2.232 loss: 2.866 loss_x: 2.865 loss_u: 0.001 mask_prob: 0.003 pseudo_acc: 0.018 \n",
            "2023-09-10 19:45:55,751 - INFO - root -   Train Epoch: 1/  20. Iter:  401/1024. LR: 0.0300 batch_time: 3.756 data_time: 2.230 loss: 2.864 loss_x: 2.862 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:45:58,547 - INFO - root -   Train Epoch: 1/  20. Iter:  402/1024. LR: 0.0300 batch_time: 3.754 data_time: 2.228 loss: 2.864 loss_x: 2.863 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:02,818 - INFO - root -   Train Epoch: 1/  20. Iter:  403/1024. LR: 0.0300 batch_time: 3.755 data_time: 2.229 loss: 2.861 loss_x: 2.860 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:05,932 - INFO - root -   Train Epoch: 1/  20. Iter:  404/1024. LR: 0.0300 batch_time: 3.754 data_time: 2.228 loss: 2.858 loss_x: 2.857 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:09,039 - INFO - root -   Train Epoch: 1/  20. Iter:  405/1024. LR: 0.0300 batch_time: 3.752 data_time: 2.226 loss: 2.859 loss_x: 2.858 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:11,851 - INFO - root -   Train Epoch: 1/  20. Iter:  406/1024. LR: 0.0300 batch_time: 3.750 data_time: 2.224 loss: 2.859 loss_x: 2.858 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:15,403 - INFO - root -   Train Epoch: 1/  20. Iter:  407/1024. LR: 0.0300 batch_time: 3.749 data_time: 2.223 loss: 2.858 loss_x: 2.857 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:18,641 - INFO - root -   Train Epoch: 1/  20. Iter:  408/1024. LR: 0.0300 batch_time: 3.748 data_time: 2.222 loss: 2.856 loss_x: 2.855 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:21,364 - INFO - root -   Train Epoch: 1/  20. Iter:  409/1024. LR: 0.0300 batch_time: 3.745 data_time: 2.219 loss: 2.854 loss_x: 2.853 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:24,188 - INFO - root -   Train Epoch: 1/  20. Iter:  410/1024. LR: 0.0300 batch_time: 3.743 data_time: 2.217 loss: 2.855 loss_x: 2.853 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:26,973 - INFO - root -   Train Epoch: 1/  20. Iter:  411/1024. LR: 0.0300 batch_time: 3.741 data_time: 2.215 loss: 2.852 loss_x: 2.851 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:30,801 - INFO - root -   Train Epoch: 1/  20. Iter:  412/1024. LR: 0.0300 batch_time: 3.741 data_time: 2.215 loss: 2.852 loss_x: 2.850 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:33,669 - INFO - root -   Train Epoch: 1/  20. Iter:  413/1024. LR: 0.0300 batch_time: 3.739 data_time: 2.213 loss: 2.851 loss_x: 2.850 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:36,629 - INFO - root -   Train Epoch: 1/  20. Iter:  414/1024. LR: 0.0300 batch_time: 3.737 data_time: 2.211 loss: 2.850 loss_x: 2.849 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:39,431 - INFO - root -   Train Epoch: 1/  20. Iter:  415/1024. LR: 0.0300 batch_time: 3.735 data_time: 2.209 loss: 2.847 loss_x: 2.846 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:42,233 - INFO - root -   Train Epoch: 1/  20. Iter:  416/1024. LR: 0.0300 batch_time: 3.732 data_time: 2.207 loss: 2.847 loss_x: 2.846 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:46,105 - INFO - root -   Train Epoch: 1/  20. Iter:  417/1024. LR: 0.0300 batch_time: 3.733 data_time: 2.207 loss: 2.846 loss_x: 2.844 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:49,124 - INFO - root -   Train Epoch: 1/  20. Iter:  418/1024. LR: 0.0300 batch_time: 3.731 data_time: 2.205 loss: 2.844 loss_x: 2.843 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:52,232 - INFO - root -   Train Epoch: 1/  20. Iter:  419/1024. LR: 0.0300 batch_time: 3.730 data_time: 2.204 loss: 2.845 loss_x: 2.843 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:55,040 - INFO - root -   Train Epoch: 1/  20. Iter:  420/1024. LR: 0.0300 batch_time: 3.727 data_time: 2.202 loss: 2.843 loss_x: 2.842 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:46:59,350 - INFO - root -   Train Epoch: 1/  20. Iter:  421/1024. LR: 0.0300 batch_time: 3.729 data_time: 2.203 loss: 2.845 loss_x: 2.844 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:47:02,531 - INFO - root -   Train Epoch: 1/  20. Iter:  422/1024. LR: 0.0300 batch_time: 3.728 data_time: 2.202 loss: 2.843 loss_x: 2.842 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:47:05,479 - INFO - root -   Train Epoch: 1/  20. Iter:  423/1024. LR: 0.0300 batch_time: 3.726 data_time: 2.200 loss: 2.840 loss_x: 2.839 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:47:08,417 - INFO - root -   Train Epoch: 1/  20. Iter:  424/1024. LR: 0.0300 batch_time: 3.724 data_time: 2.198 loss: 2.840 loss_x: 2.839 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.017 \n",
            "2023-09-10 19:47:11,635 - INFO - root -   Train Epoch: 1/  20. Iter:  425/1024. LR: 0.0300 batch_time: 3.723 data_time: 2.197 loss: 2.838 loss_x: 2.836 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:15,611 - INFO - root -   Train Epoch: 1/  20. Iter:  426/1024. LR: 0.0300 batch_time: 3.723 data_time: 2.198 loss: 2.837 loss_x: 2.836 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:18,663 - INFO - root -   Train Epoch: 1/  20. Iter:  427/1024. LR: 0.0300 batch_time: 3.722 data_time: 2.196 loss: 2.837 loss_x: 2.836 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:21,597 - INFO - root -   Train Epoch: 1/  20. Iter:  428/1024. LR: 0.0300 batch_time: 3.720 data_time: 2.194 loss: 2.837 loss_x: 2.836 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:24,463 - INFO - root -   Train Epoch: 1/  20. Iter:  429/1024. LR: 0.0300 batch_time: 3.718 data_time: 2.192 loss: 2.836 loss_x: 2.835 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:28,290 - INFO - root -   Train Epoch: 1/  20. Iter:  430/1024. LR: 0.0300 batch_time: 3.718 data_time: 2.193 loss: 2.834 loss_x: 2.833 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:31,293 - INFO - root -   Train Epoch: 1/  20. Iter:  431/1024. LR: 0.0300 batch_time: 3.716 data_time: 2.191 loss: 2.833 loss_x: 2.832 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:33,992 - INFO - root -   Train Epoch: 1/  20. Iter:  432/1024. LR: 0.0300 batch_time: 3.714 data_time: 2.189 loss: 2.833 loss_x: 2.832 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:37,119 - INFO - root -   Train Epoch: 1/  20. Iter:  433/1024. LR: 0.0300 batch_time: 3.713 data_time: 2.187 loss: 2.834 loss_x: 2.833 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:40,010 - INFO - root -   Train Epoch: 1/  20. Iter:  434/1024. LR: 0.0300 batch_time: 3.711 data_time: 2.185 loss: 2.833 loss_x: 2.832 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:44,208 - INFO - root -   Train Epoch: 1/  20. Iter:  435/1024. LR: 0.0300 batch_time: 3.712 data_time: 2.187 loss: 2.832 loss_x: 2.831 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:46,918 - INFO - root -   Train Epoch: 1/  20. Iter:  436/1024. LR: 0.0300 batch_time: 3.710 data_time: 2.184 loss: 2.832 loss_x: 2.831 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:50,014 - INFO - root -   Train Epoch: 1/  20. Iter:  437/1024. LR: 0.0300 batch_time: 3.708 data_time: 2.183 loss: 2.832 loss_x: 2.831 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:52,698 - INFO - root -   Train Epoch: 1/  20. Iter:  438/1024. LR: 0.0300 batch_time: 3.706 data_time: 2.181 loss: 2.833 loss_x: 2.832 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:47:56,305 - INFO - root -   Train Epoch: 1/  20. Iter:  439/1024. LR: 0.0300 batch_time: 3.706 data_time: 2.180 loss: 2.833 loss_x: 2.832 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:00,145 - INFO - root -   Train Epoch: 1/  20. Iter:  440/1024. LR: 0.0300 batch_time: 3.706 data_time: 2.181 loss: 2.832 loss_x: 2.831 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:03,240 - INFO - root -   Train Epoch: 1/  20. Iter:  441/1024. LR: 0.0300 batch_time: 3.705 data_time: 2.179 loss: 2.832 loss_x: 2.831 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:06,307 - INFO - root -   Train Epoch: 1/  20. Iter:  442/1024. LR: 0.0300 batch_time: 3.703 data_time: 2.178 loss: 2.831 loss_x: 2.830 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:09,460 - INFO - root -   Train Epoch: 1/  20. Iter:  443/1024. LR: 0.0300 batch_time: 3.702 data_time: 2.177 loss: 2.831 loss_x: 2.830 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:13,564 - INFO - root -   Train Epoch: 1/  20. Iter:  444/1024. LR: 0.0300 batch_time: 3.703 data_time: 2.177 loss: 2.833 loss_x: 2.832 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:16,631 - INFO - root -   Train Epoch: 1/  20. Iter:  445/1024. LR: 0.0300 batch_time: 3.701 data_time: 2.176 loss: 2.833 loss_x: 2.832 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:19,737 - INFO - root -   Train Epoch: 1/  20. Iter:  446/1024. LR: 0.0300 batch_time: 3.700 data_time: 2.175 loss: 2.833 loss_x: 2.831 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:22,829 - INFO - root -   Train Epoch: 1/  20. Iter:  447/1024. LR: 0.0300 batch_time: 3.699 data_time: 2.173 loss: 2.833 loss_x: 2.832 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:26,898 - INFO - root -   Train Epoch: 1/  20. Iter:  448/1024. LR: 0.0300 batch_time: 3.700 data_time: 2.174 loss: 2.831 loss_x: 2.830 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:29,771 - INFO - root -   Train Epoch: 1/  20. Iter:  449/1024. LR: 0.0300 batch_time: 3.698 data_time: 2.172 loss: 2.831 loss_x: 2.829 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:32,594 - INFO - root -   Train Epoch: 1/  20. Iter:  450/1024. LR: 0.0300 batch_time: 3.696 data_time: 2.171 loss: 2.830 loss_x: 2.829 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:35,541 - INFO - root -   Train Epoch: 1/  20. Iter:  451/1024. LR: 0.0300 batch_time: 3.694 data_time: 2.169 loss: 2.830 loss_x: 2.829 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:48:38,237 - INFO - root -   Train Epoch: 1/  20. Iter:  452/1024. LR: 0.0300 batch_time: 3.692 data_time: 2.167 loss: 2.829 loss_x: 2.828 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:48:42,663 - INFO - root -   Train Epoch: 1/  20. Iter:  453/1024. LR: 0.0300 batch_time: 3.693 data_time: 2.168 loss: 2.829 loss_x: 2.828 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:48:45,718 - INFO - root -   Train Epoch: 1/  20. Iter:  454/1024. LR: 0.0300 batch_time: 3.692 data_time: 2.167 loss: 2.828 loss_x: 2.827 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:48:48,447 - INFO - root -   Train Epoch: 1/  20. Iter:  455/1024. LR: 0.0300 batch_time: 3.690 data_time: 2.165 loss: 2.826 loss_x: 2.825 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:48:51,171 - INFO - root -   Train Epoch: 1/  20. Iter:  456/1024. LR: 0.0300 batch_time: 3.688 data_time: 2.163 loss: 2.826 loss_x: 2.824 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:48:54,964 - INFO - root -   Train Epoch: 1/  20. Iter:  457/1024. LR: 0.0300 batch_time: 3.688 data_time: 2.163 loss: 2.825 loss_x: 2.824 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:48:58,662 - INFO - root -   Train Epoch: 1/  20. Iter:  458/1024. LR: 0.0300 batch_time: 3.688 data_time: 2.163 loss: 2.826 loss_x: 2.825 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:01,396 - INFO - root -   Train Epoch: 1/  20. Iter:  459/1024. LR: 0.0300 batch_time: 3.686 data_time: 2.161 loss: 2.827 loss_x: 2.826 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:04,207 - INFO - root -   Train Epoch: 1/  20. Iter:  460/1024. LR: 0.0300 batch_time: 3.684 data_time: 2.159 loss: 2.827 loss_x: 2.825 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:07,314 - INFO - root -   Train Epoch: 1/  20. Iter:  461/1024. LR: 0.0300 batch_time: 3.683 data_time: 2.158 loss: 2.824 loss_x: 2.822 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:11,005 - INFO - root -   Train Epoch: 1/  20. Iter:  462/1024. LR: 0.0300 batch_time: 3.683 data_time: 2.158 loss: 2.823 loss_x: 2.822 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:14,329 - INFO - root -   Train Epoch: 1/  20. Iter:  463/1024. LR: 0.0300 batch_time: 3.682 data_time: 2.157 loss: 2.822 loss_x: 2.821 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:17,264 - INFO - root -   Train Epoch: 1/  20. Iter:  464/1024. LR: 0.0300 batch_time: 3.680 data_time: 2.155 loss: 2.821 loss_x: 2.820 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:20,067 - INFO - root -   Train Epoch: 1/  20. Iter:  465/1024. LR: 0.0300 batch_time: 3.679 data_time: 2.154 loss: 2.821 loss_x: 2.820 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:23,056 - INFO - root -   Train Epoch: 1/  20. Iter:  466/1024. LR: 0.0300 batch_time: 3.677 data_time: 2.152 loss: 2.821 loss_x: 2.820 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:26,593 - INFO - root -   Train Epoch: 1/  20. Iter:  467/1024. LR: 0.0300 batch_time: 3.677 data_time: 2.152 loss: 2.821 loss_x: 2.820 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:29,299 - INFO - root -   Train Epoch: 1/  20. Iter:  468/1024. LR: 0.0300 batch_time: 3.675 data_time: 2.150 loss: 2.819 loss_x: 2.818 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:31,993 - INFO - root -   Train Epoch: 1/  20. Iter:  469/1024. LR: 0.0300 batch_time: 3.673 data_time: 2.148 loss: 2.818 loss_x: 2.816 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:34,854 - INFO - root -   Train Epoch: 1/  20. Iter:  470/1024. LR: 0.0300 batch_time: 3.671 data_time: 2.146 loss: 2.818 loss_x: 2.816 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:38,717 - INFO - root -   Train Epoch: 1/  20. Iter:  471/1024. LR: 0.0300 batch_time: 3.671 data_time: 2.146 loss: 2.817 loss_x: 2.816 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:42,699 - INFO - root -   Train Epoch: 1/  20. Iter:  472/1024. LR: 0.0300 batch_time: 3.672 data_time: 2.147 loss: 2.815 loss_x: 2.814 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:45,676 - INFO - root -   Train Epoch: 1/  20. Iter:  473/1024. LR: 0.0300 batch_time: 3.671 data_time: 2.146 loss: 2.814 loss_x: 2.813 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:48,505 - INFO - root -   Train Epoch: 1/  20. Iter:  474/1024. LR: 0.0300 batch_time: 3.669 data_time: 2.144 loss: 2.812 loss_x: 2.811 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:51,248 - INFO - root -   Train Epoch: 1/  20. Iter:  475/1024. LR: 0.0300 batch_time: 3.667 data_time: 2.142 loss: 2.811 loss_x: 2.810 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:55,366 - INFO - root -   Train Epoch: 1/  20. Iter:  476/1024. LR: 0.0300 batch_time: 3.668 data_time: 2.143 loss: 2.810 loss_x: 2.809 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:49:58,255 - INFO - root -   Train Epoch: 1/  20. Iter:  477/1024. LR: 0.0300 batch_time: 3.666 data_time: 2.141 loss: 2.810 loss_x: 2.808 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:50:01,526 - INFO - root -   Train Epoch: 1/  20. Iter:  478/1024. LR: 0.0300 batch_time: 3.665 data_time: 2.140 loss: 2.807 loss_x: 2.806 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:50:04,351 - INFO - root -   Train Epoch: 1/  20. Iter:  479/1024. LR: 0.0300 batch_time: 3.664 data_time: 2.139 loss: 2.805 loss_x: 2.804 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:50:07,478 - INFO - root -   Train Epoch: 1/  20. Iter:  480/1024. LR: 0.0300 batch_time: 3.662 data_time: 2.138 loss: 2.807 loss_x: 2.806 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:50:11,279 - INFO - root -   Train Epoch: 1/  20. Iter:  481/1024. LR: 0.0300 batch_time: 3.663 data_time: 2.138 loss: 2.805 loss_x: 2.804 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:50:14,382 - INFO - root -   Train Epoch: 1/  20. Iter:  482/1024. LR: 0.0300 batch_time: 3.662 data_time: 2.137 loss: 2.805 loss_x: 2.803 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:50:17,107 - INFO - root -   Train Epoch: 1/  20. Iter:  483/1024. LR: 0.0300 batch_time: 3.660 data_time: 2.135 loss: 2.803 loss_x: 2.802 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:20,214 - INFO - root -   Train Epoch: 1/  20. Iter:  484/1024. LR: 0.0300 batch_time: 3.658 data_time: 2.134 loss: 2.803 loss_x: 2.802 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:23,993 - INFO - root -   Train Epoch: 1/  20. Iter:  485/1024. LR: 0.0300 batch_time: 3.659 data_time: 2.134 loss: 2.802 loss_x: 2.801 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:27,534 - INFO - root -   Train Epoch: 1/  20. Iter:  486/1024. LR: 0.0300 batch_time: 3.658 data_time: 2.133 loss: 2.802 loss_x: 2.801 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:30,671 - INFO - root -   Train Epoch: 1/  20. Iter:  487/1024. LR: 0.0300 batch_time: 3.657 data_time: 2.132 loss: 2.800 loss_x: 2.799 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:33,694 - INFO - root -   Train Epoch: 1/  20. Iter:  488/1024. LR: 0.0300 batch_time: 3.656 data_time: 2.131 loss: 2.797 loss_x: 2.796 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:36,685 - INFO - root -   Train Epoch: 1/  20. Iter:  489/1024. LR: 0.0300 batch_time: 3.655 data_time: 2.130 loss: 2.797 loss_x: 2.796 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:40,749 - INFO - root -   Train Epoch: 1/  20. Iter:  490/1024. LR: 0.0300 batch_time: 3.656 data_time: 2.131 loss: 2.797 loss_x: 2.796 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:43,561 - INFO - root -   Train Epoch: 1/  20. Iter:  491/1024. LR: 0.0300 batch_time: 3.654 data_time: 2.129 loss: 2.796 loss_x: 2.795 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:46,694 - INFO - root -   Train Epoch: 1/  20. Iter:  492/1024. LR: 0.0300 batch_time: 3.653 data_time: 2.128 loss: 2.796 loss_x: 2.795 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:49,678 - INFO - root -   Train Epoch: 1/  20. Iter:  493/1024. LR: 0.0300 batch_time: 3.651 data_time: 2.127 loss: 2.795 loss_x: 2.794 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:53,842 - INFO - root -   Train Epoch: 1/  20. Iter:  494/1024. LR: 0.0300 batch_time: 3.652 data_time: 2.128 loss: 2.794 loss_x: 2.793 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:50:57,090 - INFO - root -   Train Epoch: 1/  20. Iter:  495/1024. LR: 0.0300 batch_time: 3.652 data_time: 2.127 loss: 2.794 loss_x: 2.792 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:00,177 - INFO - root -   Train Epoch: 1/  20. Iter:  496/1024. LR: 0.0300 batch_time: 3.651 data_time: 2.126 loss: 2.793 loss_x: 2.792 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:03,122 - INFO - root -   Train Epoch: 1/  20. Iter:  497/1024. LR: 0.0300 batch_time: 3.649 data_time: 2.124 loss: 2.791 loss_x: 2.790 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:06,373 - INFO - root -   Train Epoch: 1/  20. Iter:  498/1024. LR: 0.0300 batch_time: 3.648 data_time: 2.123 loss: 2.789 loss_x: 2.788 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:10,416 - INFO - root -   Train Epoch: 1/  20. Iter:  499/1024. LR: 0.0300 batch_time: 3.649 data_time: 2.124 loss: 2.790 loss_x: 2.789 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:13,533 - INFO - root -   Train Epoch: 1/  20. Iter:  500/1024. LR: 0.0300 batch_time: 3.648 data_time: 2.123 loss: 2.789 loss_x: 2.787 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:16,632 - INFO - root -   Train Epoch: 1/  20. Iter:  501/1024. LR: 0.0300 batch_time: 3.647 data_time: 2.122 loss: 2.791 loss_x: 2.790 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:19,448 - INFO - root -   Train Epoch: 1/  20. Iter:  502/1024. LR: 0.0300 batch_time: 3.645 data_time: 2.121 loss: 2.789 loss_x: 2.788 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:23,290 - INFO - root -   Train Epoch: 1/  20. Iter:  503/1024. LR: 0.0300 batch_time: 3.646 data_time: 2.121 loss: 2.789 loss_x: 2.788 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:26,475 - INFO - root -   Train Epoch: 1/  20. Iter:  504/1024. LR: 0.0300 batch_time: 3.645 data_time: 2.120 loss: 2.789 loss_x: 2.788 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:29,405 - INFO - root -   Train Epoch: 1/  20. Iter:  505/1024. LR: 0.0300 batch_time: 3.643 data_time: 2.119 loss: 2.790 loss_x: 2.789 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:32,468 - INFO - root -   Train Epoch: 1/  20. Iter:  506/1024. LR: 0.0300 batch_time: 3.642 data_time: 2.118 loss: 2.789 loss_x: 2.788 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:35,212 - INFO - root -   Train Epoch: 1/  20. Iter:  507/1024. LR: 0.0300 batch_time: 3.640 data_time: 2.116 loss: 2.790 loss_x: 2.789 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:39,391 - INFO - root -   Train Epoch: 1/  20. Iter:  508/1024. LR: 0.0300 batch_time: 3.641 data_time: 2.117 loss: 2.790 loss_x: 2.789 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:42,139 - INFO - root -   Train Epoch: 1/  20. Iter:  509/1024. LR: 0.0300 batch_time: 3.640 data_time: 2.115 loss: 2.788 loss_x: 2.787 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:45,248 - INFO - root -   Train Epoch: 1/  20. Iter:  510/1024. LR: 0.0300 batch_time: 3.639 data_time: 2.114 loss: 2.788 loss_x: 2.787 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:48,013 - INFO - root -   Train Epoch: 1/  20. Iter:  511/1024. LR: 0.0300 batch_time: 3.637 data_time: 2.112 loss: 2.786 loss_x: 2.784 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:51,856 - INFO - root -   Train Epoch: 1/  20. Iter:  512/1024. LR: 0.0300 batch_time: 3.637 data_time: 2.113 loss: 2.784 loss_x: 2.783 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:55,301 - INFO - root -   Train Epoch: 1/  20. Iter:  513/1024. LR: 0.0300 batch_time: 3.637 data_time: 2.112 loss: 2.782 loss_x: 2.781 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:51:58,226 - INFO - root -   Train Epoch: 1/  20. Iter:  514/1024. LR: 0.0300 batch_time: 3.636 data_time: 2.111 loss: 2.781 loss_x: 2.780 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:52:01,221 - INFO - root -   Train Epoch: 1/  20. Iter:  515/1024. LR: 0.0300 batch_time: 3.634 data_time: 2.110 loss: 2.781 loss_x: 2.780 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:52:03,901 - INFO - root -   Train Epoch: 1/  20. Iter:  516/1024. LR: 0.0300 batch_time: 3.633 data_time: 2.108 loss: 2.780 loss_x: 2.779 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:52:07,764 - INFO - root -   Train Epoch: 1/  20. Iter:  517/1024. LR: 0.0300 batch_time: 3.633 data_time: 2.108 loss: 2.779 loss_x: 2.778 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:52:10,734 - INFO - root -   Train Epoch: 1/  20. Iter:  518/1024. LR: 0.0300 batch_time: 3.632 data_time: 2.107 loss: 2.778 loss_x: 2.777 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:52:13,843 - INFO - root -   Train Epoch: 1/  20. Iter:  519/1024. LR: 0.0300 batch_time: 3.631 data_time: 2.106 loss: 2.776 loss_x: 2.775 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:16,600 - INFO - root -   Train Epoch: 1/  20. Iter:  520/1024. LR: 0.0300 batch_time: 3.629 data_time: 2.104 loss: 2.776 loss_x: 2.775 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:19,855 - INFO - root -   Train Epoch: 1/  20. Iter:  521/1024. LR: 0.0300 batch_time: 3.628 data_time: 2.104 loss: 2.774 loss_x: 2.773 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:23,512 - INFO - root -   Train Epoch: 1/  20. Iter:  522/1024. LR: 0.0300 batch_time: 3.628 data_time: 2.104 loss: 2.773 loss_x: 2.772 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:26,233 - INFO - root -   Train Epoch: 1/  20. Iter:  523/1024. LR: 0.0300 batch_time: 3.627 data_time: 2.102 loss: 2.771 loss_x: 2.770 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:29,164 - INFO - root -   Train Epoch: 1/  20. Iter:  524/1024. LR: 0.0300 batch_time: 3.625 data_time: 2.101 loss: 2.769 loss_x: 2.768 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:32,172 - INFO - root -   Train Epoch: 1/  20. Iter:  525/1024. LR: 0.0300 batch_time: 3.624 data_time: 2.100 loss: 2.768 loss_x: 2.767 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:36,254 - INFO - root -   Train Epoch: 1/  20. Iter:  526/1024. LR: 0.0300 batch_time: 3.625 data_time: 2.100 loss: 2.768 loss_x: 2.767 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:39,301 - INFO - root -   Train Epoch: 1/  20. Iter:  527/1024. LR: 0.0300 batch_time: 3.624 data_time: 2.099 loss: 2.766 loss_x: 2.765 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:42,468 - INFO - root -   Train Epoch: 1/  20. Iter:  528/1024. LR: 0.0300 batch_time: 3.623 data_time: 2.098 loss: 2.765 loss_x: 2.764 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:45,567 - INFO - root -   Train Epoch: 1/  20. Iter:  529/1024. LR: 0.0300 batch_time: 3.622 data_time: 2.098 loss: 2.765 loss_x: 2.764 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:49,130 - INFO - root -   Train Epoch: 1/  20. Iter:  530/1024. LR: 0.0300 batch_time: 3.622 data_time: 2.097 loss: 2.766 loss_x: 2.765 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:52,713 - INFO - root -   Train Epoch: 1/  20. Iter:  531/1024. LR: 0.0300 batch_time: 3.622 data_time: 2.097 loss: 2.766 loss_x: 2.765 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:55,350 - INFO - root -   Train Epoch: 1/  20. Iter:  532/1024. LR: 0.0300 batch_time: 3.620 data_time: 2.095 loss: 2.763 loss_x: 2.762 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:52:58,303 - INFO - root -   Train Epoch: 1/  20. Iter:  533/1024. LR: 0.0300 batch_time: 3.619 data_time: 2.094 loss: 2.763 loss_x: 2.762 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:01,462 - INFO - root -   Train Epoch: 1/  20. Iter:  534/1024. LR: 0.0300 batch_time: 3.618 data_time: 2.093 loss: 2.763 loss_x: 2.762 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:05,893 - INFO - root -   Train Epoch: 1/  20. Iter:  535/1024. LR: 0.0300 batch_time: 3.619 data_time: 2.095 loss: 2.763 loss_x: 2.762 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:08,777 - INFO - root -   Train Epoch: 1/  20. Iter:  536/1024. LR: 0.0300 batch_time: 3.618 data_time: 2.094 loss: 2.761 loss_x: 2.760 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:11,557 - INFO - root -   Train Epoch: 1/  20. Iter:  537/1024. LR: 0.0300 batch_time: 3.616 data_time: 2.092 loss: 2.761 loss_x: 2.760 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:14,517 - INFO - root -   Train Epoch: 1/  20. Iter:  538/1024. LR: 0.0300 batch_time: 3.615 data_time: 2.091 loss: 2.759 loss_x: 2.758 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:17,939 - INFO - root -   Train Epoch: 1/  20. Iter:  539/1024. LR: 0.0300 batch_time: 3.615 data_time: 2.090 loss: 2.758 loss_x: 2.757 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:22,041 - INFO - root -   Train Epoch: 1/  20. Iter:  540/1024. LR: 0.0300 batch_time: 3.616 data_time: 2.091 loss: 2.758 loss_x: 2.757 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:24,880 - INFO - root -   Train Epoch: 1/  20. Iter:  541/1024. LR: 0.0300 batch_time: 3.614 data_time: 2.090 loss: 2.757 loss_x: 2.756 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:27,801 - INFO - root -   Train Epoch: 1/  20. Iter:  542/1024. LR: 0.0300 batch_time: 3.613 data_time: 2.089 loss: 2.755 loss_x: 2.754 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:30,696 - INFO - root -   Train Epoch: 1/  20. Iter:  543/1024. LR: 0.0300 batch_time: 3.612 data_time: 2.087 loss: 2.754 loss_x: 2.753 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:34,630 - INFO - root -   Train Epoch: 1/  20. Iter:  544/1024. LR: 0.0300 batch_time: 3.612 data_time: 2.088 loss: 2.753 loss_x: 2.752 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:37,388 - INFO - root -   Train Epoch: 1/  20. Iter:  545/1024. LR: 0.0300 batch_time: 3.611 data_time: 2.086 loss: 2.753 loss_x: 2.752 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:40,355 - INFO - root -   Train Epoch: 1/  20. Iter:  546/1024. LR: 0.0300 batch_time: 3.610 data_time: 2.085 loss: 2.752 loss_x: 2.751 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:43,457 - INFO - root -   Train Epoch: 1/  20. Iter:  547/1024. LR: 0.0300 batch_time: 3.609 data_time: 2.084 loss: 2.752 loss_x: 2.751 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:46,386 - INFO - root -   Train Epoch: 1/  20. Iter:  548/1024. LR: 0.0300 batch_time: 3.607 data_time: 2.083 loss: 2.754 loss_x: 2.753 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:50,361 - INFO - root -   Train Epoch: 1/  20. Iter:  549/1024. LR: 0.0300 batch_time: 3.608 data_time: 2.084 loss: 2.753 loss_x: 2.752 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:53,457 - INFO - root -   Train Epoch: 1/  20. Iter:  550/1024. LR: 0.0300 batch_time: 3.607 data_time: 2.083 loss: 2.751 loss_x: 2.750 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:56,582 - INFO - root -   Train Epoch: 1/  20. Iter:  551/1024. LR: 0.0300 batch_time: 3.606 data_time: 2.082 loss: 2.751 loss_x: 2.750 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:53:59,736 - INFO - root -   Train Epoch: 1/  20. Iter:  552/1024. LR: 0.0300 batch_time: 3.605 data_time: 2.081 loss: 2.750 loss_x: 2.749 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:54:04,159 - INFO - root -   Train Epoch: 1/  20. Iter:  553/1024. LR: 0.0300 batch_time: 3.607 data_time: 2.082 loss: 2.751 loss_x: 2.750 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:54:07,146 - INFO - root -   Train Epoch: 1/  20. Iter:  554/1024. LR: 0.0300 batch_time: 3.606 data_time: 2.081 loss: 2.750 loss_x: 2.749 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:54:10,107 - INFO - root -   Train Epoch: 1/  20. Iter:  555/1024. LR: 0.0300 batch_time: 3.605 data_time: 2.080 loss: 2.749 loss_x: 2.748 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:54:12,911 - INFO - root -   Train Epoch: 1/  20. Iter:  556/1024. LR: 0.0300 batch_time: 3.603 data_time: 2.079 loss: 2.750 loss_x: 2.749 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.013 \n",
            "2023-09-10 19:54:16,647 - INFO - root -   Train Epoch: 1/  20. Iter:  557/1024. LR: 0.0300 batch_time: 3.603 data_time: 2.079 loss: 2.749 loss_x: 2.748 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:20,597 - INFO - root -   Train Epoch: 1/  20. Iter:  558/1024. LR: 0.0300 batch_time: 3.604 data_time: 2.080 loss: 2.749 loss_x: 2.748 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:23,427 - INFO - root -   Train Epoch: 1/  20. Iter:  559/1024. LR: 0.0300 batch_time: 3.603 data_time: 2.078 loss: 2.748 loss_x: 2.747 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:26,128 - INFO - root -   Train Epoch: 1/  20. Iter:  560/1024. LR: 0.0300 batch_time: 3.601 data_time: 2.077 loss: 2.747 loss_x: 2.746 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:29,243 - INFO - root -   Train Epoch: 1/  20. Iter:  561/1024. LR: 0.0300 batch_time: 3.600 data_time: 2.076 loss: 2.746 loss_x: 2.745 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:33,502 - INFO - root -   Train Epoch: 1/  20. Iter:  562/1024. LR: 0.0300 batch_time: 3.601 data_time: 2.077 loss: 2.746 loss_x: 2.744 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:36,438 - INFO - root -   Train Epoch: 1/  20. Iter:  563/1024. LR: 0.0300 batch_time: 3.600 data_time: 2.076 loss: 2.745 loss_x: 2.744 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:39,604 - INFO - root -   Train Epoch: 1/  20. Iter:  564/1024. LR: 0.0300 batch_time: 3.599 data_time: 2.075 loss: 2.743 loss_x: 2.742 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:42,661 - INFO - root -   Train Epoch: 1/  20. Iter:  565/1024. LR: 0.0300 batch_time: 3.598 data_time: 2.074 loss: 2.742 loss_x: 2.741 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:46,552 - INFO - root -   Train Epoch: 1/  20. Iter:  566/1024. LR: 0.0300 batch_time: 3.599 data_time: 2.075 loss: 2.742 loss_x: 2.740 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:50,105 - INFO - root -   Train Epoch: 1/  20. Iter:  567/1024. LR: 0.0300 batch_time: 3.599 data_time: 2.074 loss: 2.741 loss_x: 2.740 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:53,256 - INFO - root -   Train Epoch: 1/  20. Iter:  568/1024. LR: 0.0300 batch_time: 3.598 data_time: 2.074 loss: 2.740 loss_x: 2.739 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:56,043 - INFO - root -   Train Epoch: 1/  20. Iter:  569/1024. LR: 0.0300 batch_time: 3.597 data_time: 2.072 loss: 2.741 loss_x: 2.739 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:54:58,957 - INFO - root -   Train Epoch: 1/  20. Iter:  570/1024. LR: 0.0300 batch_time: 3.596 data_time: 2.071 loss: 2.741 loss_x: 2.740 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:55:03,453 - INFO - root -   Train Epoch: 1/  20. Iter:  571/1024. LR: 0.0300 batch_time: 3.597 data_time: 2.073 loss: 2.740 loss_x: 2.739 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:55:06,659 - INFO - root -   Train Epoch: 1/  20. Iter:  572/1024. LR: 0.0300 batch_time: 3.596 data_time: 2.072 loss: 2.739 loss_x: 2.738 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:09,532 - INFO - root -   Train Epoch: 1/  20. Iter:  573/1024. LR: 0.0300 batch_time: 3.595 data_time: 2.071 loss: 2.737 loss_x: 2.736 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:12,360 - INFO - root -   Train Epoch: 1/  20. Iter:  574/1024. LR: 0.0300 batch_time: 3.594 data_time: 2.070 loss: 2.737 loss_x: 2.736 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:16,212 - INFO - root -   Train Epoch: 1/  20. Iter:  575/1024. LR: 0.0300 batch_time: 3.594 data_time: 2.070 loss: 2.737 loss_x: 2.736 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:19,885 - INFO - root -   Train Epoch: 1/  20. Iter:  576/1024. LR: 0.0300 batch_time: 3.594 data_time: 2.070 loss: 2.736 loss_x: 2.735 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:22,876 - INFO - root -   Train Epoch: 1/  20. Iter:  577/1024. LR: 0.0300 batch_time: 3.593 data_time: 2.069 loss: 2.737 loss_x: 2.735 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:25,847 - INFO - root -   Train Epoch: 1/  20. Iter:  578/1024. LR: 0.0300 batch_time: 3.592 data_time: 2.068 loss: 2.736 loss_x: 2.735 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:28,813 - INFO - root -   Train Epoch: 1/  20. Iter:  579/1024. LR: 0.0300 batch_time: 3.591 data_time: 2.067 loss: 2.735 loss_x: 2.734 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:32,612 - INFO - root -   Train Epoch: 1/  20. Iter:  580/1024. LR: 0.0300 batch_time: 3.592 data_time: 2.067 loss: 2.734 loss_x: 2.733 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.016 \n",
            "2023-09-10 19:55:35,483 - INFO - root -   Train Epoch: 1/  20. Iter:  581/1024. LR: 0.0300 batch_time: 3.590 data_time: 2.066 loss: 2.733 loss_x: 2.732 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:55:38,214 - INFO - root -   Train Epoch: 1/  20. Iter:  582/1024. LR: 0.0300 batch_time: 3.589 data_time: 2.064 loss: 2.731 loss_x: 2.730 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:55:41,158 - INFO - root -   Train Epoch: 1/  20. Iter:  583/1024. LR: 0.0300 batch_time: 3.588 data_time: 2.063 loss: 2.729 loss_x: 2.728 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:55:44,333 - INFO - root -   Train Epoch: 1/  20. Iter:  584/1024. LR: 0.0300 batch_time: 3.587 data_time: 2.063 loss: 2.728 loss_x: 2.727 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:55:48,510 - INFO - root -   Train Epoch: 1/  20. Iter:  585/1024. LR: 0.0300 batch_time: 3.588 data_time: 2.064 loss: 2.729 loss_x: 2.728 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:55:51,283 - INFO - root -   Train Epoch: 1/  20. Iter:  586/1024. LR: 0.0300 batch_time: 3.587 data_time: 2.062 loss: 2.728 loss_x: 2.727 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:55:54,009 - INFO - root -   Train Epoch: 1/  20. Iter:  587/1024. LR: 0.0300 batch_time: 3.585 data_time: 2.061 loss: 2.730 loss_x: 2.729 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:55:57,102 - INFO - root -   Train Epoch: 1/  20. Iter:  588/1024. LR: 0.0300 batch_time: 3.584 data_time: 2.060 loss: 2.729 loss_x: 2.728 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:01,595 - INFO - root -   Train Epoch: 1/  20. Iter:  589/1024. LR: 0.0300 batch_time: 3.586 data_time: 2.062 loss: 2.728 loss_x: 2.727 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:04,854 - INFO - root -   Train Epoch: 1/  20. Iter:  590/1024. LR: 0.0300 batch_time: 3.585 data_time: 2.061 loss: 2.728 loss_x: 2.727 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:07,620 - INFO - root -   Train Epoch: 1/  20. Iter:  591/1024. LR: 0.0300 batch_time: 3.584 data_time: 2.060 loss: 2.726 loss_x: 2.725 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:10,606 - INFO - root -   Train Epoch: 1/  20. Iter:  592/1024. LR: 0.0300 batch_time: 3.583 data_time: 2.059 loss: 2.725 loss_x: 2.724 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:14,338 - INFO - root -   Train Epoch: 1/  20. Iter:  593/1024. LR: 0.0300 batch_time: 3.583 data_time: 2.059 loss: 2.726 loss_x: 2.724 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:18,402 - INFO - root -   Train Epoch: 1/  20. Iter:  594/1024. LR: 0.0300 batch_time: 3.584 data_time: 2.060 loss: 2.723 loss_x: 2.722 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:21,211 - INFO - root -   Train Epoch: 1/  20. Iter:  595/1024. LR: 0.0300 batch_time: 3.583 data_time: 2.058 loss: 2.723 loss_x: 2.722 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:24,382 - INFO - root -   Train Epoch: 1/  20. Iter:  596/1024. LR: 0.0300 batch_time: 3.582 data_time: 2.058 loss: 2.720 loss_x: 2.719 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:27,280 - INFO - root -   Train Epoch: 1/  20. Iter:  597/1024. LR: 0.0300 batch_time: 3.581 data_time: 2.057 loss: 2.719 loss_x: 2.718 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:31,571 - INFO - root -   Train Epoch: 1/  20. Iter:  598/1024. LR: 0.0300 batch_time: 3.582 data_time: 2.058 loss: 2.719 loss_x: 2.718 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:34,784 - INFO - root -   Train Epoch: 1/  20. Iter:  599/1024. LR: 0.0300 batch_time: 3.581 data_time: 2.057 loss: 2.719 loss_x: 2.717 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:37,509 - INFO - root -   Train Epoch: 1/  20. Iter:  600/1024. LR: 0.0300 batch_time: 3.580 data_time: 2.056 loss: 2.718 loss_x: 2.717 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:40,375 - INFO - root -   Train Epoch: 1/  20. Iter:  601/1024. LR: 0.0300 batch_time: 3.579 data_time: 2.055 loss: 2.717 loss_x: 2.716 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:44,436 - INFO - root -   Train Epoch: 1/  20. Iter:  602/1024. LR: 0.0300 batch_time: 3.580 data_time: 2.055 loss: 2.716 loss_x: 2.715 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:48,364 - INFO - root -   Train Epoch: 1/  20. Iter:  603/1024. LR: 0.0300 batch_time: 3.580 data_time: 2.056 loss: 2.716 loss_x: 2.714 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:51,116 - INFO - root -   Train Epoch: 1/  20. Iter:  604/1024. LR: 0.0300 batch_time: 3.579 data_time: 2.054 loss: 2.715 loss_x: 2.714 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:54,148 - INFO - root -   Train Epoch: 1/  20. Iter:  605/1024. LR: 0.0300 batch_time: 3.578 data_time: 2.054 loss: 2.713 loss_x: 2.712 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:56:57,118 - INFO - root -   Train Epoch: 1/  20. Iter:  606/1024. LR: 0.0300 batch_time: 3.577 data_time: 2.053 loss: 2.713 loss_x: 2.712 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:01,580 - INFO - root -   Train Epoch: 1/  20. Iter:  607/1024. LR: 0.0300 batch_time: 3.578 data_time: 2.054 loss: 2.713 loss_x: 2.712 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:04,813 - INFO - root -   Train Epoch: 1/  20. Iter:  608/1024. LR: 0.0300 batch_time: 3.578 data_time: 2.053 loss: 2.714 loss_x: 2.713 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:07,839 - INFO - root -   Train Epoch: 1/  20. Iter:  609/1024. LR: 0.0300 batch_time: 3.577 data_time: 2.053 loss: 2.711 loss_x: 2.710 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:11,050 - INFO - root -   Train Epoch: 1/  20. Iter:  610/1024. LR: 0.0300 batch_time: 3.576 data_time: 2.052 loss: 2.711 loss_x: 2.710 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:15,742 - INFO - root -   Train Epoch: 1/  20. Iter:  611/1024. LR: 0.0300 batch_time: 3.578 data_time: 2.054 loss: 2.711 loss_x: 2.710 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:19,196 - INFO - root -   Train Epoch: 1/  20. Iter:  612/1024. LR: 0.0300 batch_time: 3.578 data_time: 2.054 loss: 2.709 loss_x: 2.708 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:22,479 - INFO - root -   Train Epoch: 1/  20. Iter:  613/1024. LR: 0.0300 batch_time: 3.577 data_time: 2.053 loss: 2.708 loss_x: 2.707 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:25,430 - INFO - root -   Train Epoch: 1/  20. Iter:  614/1024. LR: 0.0300 batch_time: 3.576 data_time: 2.052 loss: 2.707 loss_x: 2.706 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:28,719 - INFO - root -   Train Epoch: 1/  20. Iter:  615/1024. LR: 0.0300 batch_time: 3.576 data_time: 2.052 loss: 2.705 loss_x: 2.704 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:33,095 - INFO - root -   Train Epoch: 1/  20. Iter:  616/1024. LR: 0.0300 batch_time: 3.577 data_time: 2.053 loss: 2.705 loss_x: 2.704 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:36,049 - INFO - root -   Train Epoch: 1/  20. Iter:  617/1024. LR: 0.0300 batch_time: 3.576 data_time: 2.052 loss: 2.705 loss_x: 2.704 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:38,845 - INFO - root -   Train Epoch: 1/  20. Iter:  618/1024. LR: 0.0300 batch_time: 3.575 data_time: 2.051 loss: 2.703 loss_x: 2.702 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:41,704 - INFO - root -   Train Epoch: 1/  20. Iter:  619/1024. LR: 0.0300 batch_time: 3.574 data_time: 2.049 loss: 2.701 loss_x: 2.700 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:45,958 - INFO - root -   Train Epoch: 1/  20. Iter:  620/1024. LR: 0.0300 batch_time: 3.575 data_time: 2.050 loss: 2.702 loss_x: 2.701 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.015 \n",
            "2023-09-10 19:57:49,469 - INFO - root -   Train Epoch: 1/  20. Iter:  621/1024. LR: 0.0300 batch_time: 3.575 data_time: 2.050 loss: 2.701 loss_x: 2.700 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:57:52,766 - INFO - root -   Train Epoch: 1/  20. Iter:  622/1024. LR: 0.0300 batch_time: 3.574 data_time: 2.050 loss: 2.700 loss_x: 2.699 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:57:55,851 - INFO - root -   Train Epoch: 1/  20. Iter:  623/1024. LR: 0.0300 batch_time: 3.574 data_time: 2.049 loss: 2.699 loss_x: 2.698 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:57:59,802 - INFO - root -   Train Epoch: 1/  20. Iter:  624/1024. LR: 0.0300 batch_time: 3.574 data_time: 2.050 loss: 2.697 loss_x: 2.696 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:03,438 - INFO - root -   Train Epoch: 1/  20. Iter:  625/1024. LR: 0.0300 batch_time: 3.574 data_time: 2.050 loss: 2.697 loss_x: 2.696 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:06,632 - INFO - root -   Train Epoch: 1/  20. Iter:  626/1024. LR: 0.0300 batch_time: 3.574 data_time: 2.049 loss: 2.697 loss_x: 2.696 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:10,005 - INFO - root -   Train Epoch: 1/  20. Iter:  627/1024. LR: 0.0300 batch_time: 3.573 data_time: 2.049 loss: 2.696 loss_x: 2.695 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:12,931 - INFO - root -   Train Epoch: 1/  20. Iter:  628/1024. LR: 0.0300 batch_time: 3.572 data_time: 2.048 loss: 2.696 loss_x: 2.695 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:17,626 - INFO - root -   Train Epoch: 1/  20. Iter:  629/1024. LR: 0.0300 batch_time: 3.574 data_time: 2.049 loss: 2.696 loss_x: 2.695 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:20,496 - INFO - root -   Train Epoch: 1/  20. Iter:  630/1024. LR: 0.0300 batch_time: 3.573 data_time: 2.048 loss: 2.696 loss_x: 2.695 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:23,641 - INFO - root -   Train Epoch: 1/  20. Iter:  631/1024. LR: 0.0300 batch_time: 3.572 data_time: 2.048 loss: 2.695 loss_x: 2.694 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:26,366 - INFO - root -   Train Epoch: 1/  20. Iter:  632/1024. LR: 0.0300 batch_time: 3.571 data_time: 2.046 loss: 2.694 loss_x: 2.693 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:30,150 - INFO - root -   Train Epoch: 1/  20. Iter:  633/1024. LR: 0.0300 batch_time: 3.571 data_time: 2.047 loss: 2.693 loss_x: 2.692 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:33,425 - INFO - root -   Train Epoch: 1/  20. Iter:  634/1024. LR: 0.0300 batch_time: 3.571 data_time: 2.046 loss: 2.692 loss_x: 2.691 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:36,608 - INFO - root -   Train Epoch: 1/  20. Iter:  635/1024. LR: 0.0300 batch_time: 3.570 data_time: 2.046 loss: 2.691 loss_x: 2.690 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:39,593 - INFO - root -   Train Epoch: 1/  20. Iter:  636/1024. LR: 0.0300 batch_time: 3.569 data_time: 2.045 loss: 2.694 loss_x: 2.693 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:42,345 - INFO - root -   Train Epoch: 1/  20. Iter:  637/1024. LR: 0.0300 batch_time: 3.568 data_time: 2.043 loss: 2.692 loss_x: 2.691 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:46,681 - INFO - root -   Train Epoch: 1/  20. Iter:  638/1024. LR: 0.0300 batch_time: 3.569 data_time: 2.045 loss: 2.694 loss_x: 2.693 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:49,553 - INFO - root -   Train Epoch: 1/  20. Iter:  639/1024. LR: 0.0300 batch_time: 3.568 data_time: 2.044 loss: 2.694 loss_x: 2.693 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:52,424 - INFO - root -   Train Epoch: 1/  20. Iter:  640/1024. LR: 0.0300 batch_time: 3.567 data_time: 2.042 loss: 2.694 loss_x: 2.693 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:55,445 - INFO - root -   Train Epoch: 1/  20. Iter:  641/1024. LR: 0.0300 batch_time: 3.566 data_time: 2.042 loss: 2.695 loss_x: 2.694 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:58:58,842 - INFO - root -   Train Epoch: 1/  20. Iter:  642/1024. LR: 0.0300 batch_time: 3.566 data_time: 2.041 loss: 2.694 loss_x: 2.693 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:02,438 - INFO - root -   Train Epoch: 1/  20. Iter:  643/1024. LR: 0.0300 batch_time: 3.566 data_time: 2.041 loss: 2.693 loss_x: 2.692 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:05,336 - INFO - root -   Train Epoch: 1/  20. Iter:  644/1024. LR: 0.0300 batch_time: 3.565 data_time: 2.040 loss: 2.692 loss_x: 2.691 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:08,416 - INFO - root -   Train Epoch: 1/  20. Iter:  645/1024. LR: 0.0300 batch_time: 3.564 data_time: 2.040 loss: 2.691 loss_x: 2.690 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:11,453 - INFO - root -   Train Epoch: 1/  20. Iter:  646/1024. LR: 0.0300 batch_time: 3.563 data_time: 2.039 loss: 2.691 loss_x: 2.690 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:15,264 - INFO - root -   Train Epoch: 1/  20. Iter:  647/1024. LR: 0.0300 batch_time: 3.564 data_time: 2.039 loss: 2.691 loss_x: 2.690 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:18,270 - INFO - root -   Train Epoch: 1/  20. Iter:  648/1024. LR: 0.0300 batch_time: 3.563 data_time: 2.038 loss: 2.690 loss_x: 2.689 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:21,378 - INFO - root -   Train Epoch: 1/  20. Iter:  649/1024. LR: 0.0300 batch_time: 3.562 data_time: 2.038 loss: 2.690 loss_x: 2.689 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:24,314 - INFO - root -   Train Epoch: 1/  20. Iter:  650/1024. LR: 0.0300 batch_time: 3.561 data_time: 2.037 loss: 2.689 loss_x: 2.688 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:27,403 - INFO - root -   Train Epoch: 1/  20. Iter:  651/1024. LR: 0.0300 batch_time: 3.561 data_time: 2.036 loss: 2.688 loss_x: 2.687 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:31,933 - INFO - root -   Train Epoch: 1/  20. Iter:  652/1024. LR: 0.0300 batch_time: 3.562 data_time: 2.037 loss: 2.688 loss_x: 2.687 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:35,084 - INFO - root -   Train Epoch: 1/  20. Iter:  653/1024. LR: 0.0300 batch_time: 3.561 data_time: 2.037 loss: 2.687 loss_x: 2.686 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:38,102 - INFO - root -   Train Epoch: 1/  20. Iter:  654/1024. LR: 0.0300 batch_time: 3.561 data_time: 2.036 loss: 2.685 loss_x: 2.684 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:41,242 - INFO - root -   Train Epoch: 1/  20. Iter:  655/1024. LR: 0.0300 batch_time: 3.560 data_time: 2.035 loss: 2.686 loss_x: 2.685 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:45,431 - INFO - root -   Train Epoch: 1/  20. Iter:  656/1024. LR: 0.0300 batch_time: 3.561 data_time: 2.036 loss: 2.684 loss_x: 2.683 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:48,634 - INFO - root -   Train Epoch: 1/  20. Iter:  657/1024. LR: 0.0300 batch_time: 3.560 data_time: 2.036 loss: 2.684 loss_x: 2.683 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:51,655 - INFO - root -   Train Epoch: 1/  20. Iter:  658/1024. LR: 0.0300 batch_time: 3.559 data_time: 2.035 loss: 2.683 loss_x: 2.682 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:54,566 - INFO - root -   Train Epoch: 1/  20. Iter:  659/1024. LR: 0.0300 batch_time: 3.558 data_time: 2.034 loss: 2.682 loss_x: 2.682 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 19:59:57,895 - INFO - root -   Train Epoch: 1/  20. Iter:  660/1024. LR: 0.0300 batch_time: 3.558 data_time: 2.034 loss: 2.681 loss_x: 2.680 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 20:00:01,586 - INFO - root -   Train Epoch: 1/  20. Iter:  661/1024. LR: 0.0300 batch_time: 3.558 data_time: 2.034 loss: 2.682 loss_x: 2.681 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n",
            "2023-09-10 20:00:04,600 - INFO - root -   Train Epoch: 1/  20. Iter:  662/1024. LR: 0.0300 batch_time: 3.558 data_time: 2.033 loss: 2.684 loss_x: 2.683 loss_u: 0.001 mask_prob: 0.002 pseudo_acc: 0.014 \n"
          ]
        }
      ]
    }
  ]
}